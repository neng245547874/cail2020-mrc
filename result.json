{
  "name": "train_v1",
  "prediction_path": "output/submissions/train_v1",
  "checkpoint_path": "output/checkpoints/train_v1",
  "data_dir": "./data/output",
  "fp16": false,
  "ckpt_id": 0,
  "bert_model": "/data/wuyunzhao/pre-model/chinese_roberta_wwm_large_ext_pytorch",
  "epochs": 10,
  "qat_epochs": 0,
  "batch_size": 2,
  "max_bert_size": 8,
  "eval_batch_size": 2,
  "lr": 2e-05,
  "decay": 1.0,
  "early_stop_epoch": 0,
  "verbose_step": 50,
  "gradient_accumulation_steps": 1,
  "seed": 5,
  "q_update": false,
  "prediction_trans": false,
  "trans_drop": 0.5,
  "trans_heads": 3,
  "input_dim": 768,
  "model_gpu": "2,3",
  "trained_weight": null,
  "type_lambda": 1,
  "sp_lambda": 5,
  "sp_threshold": 0.5,
  "label_type_num": 4,
  "max_query_len": 50,
  "max_doc_len": 512
}{
 "em": 0.6752475247524753,
 "f1": 0.7789226605339139,
 "prec": 0.7838598754393595,
 "recall": 0.792999894514152,
 "sp_em": 0.3792079207920792,
 "sp_f1": 0.6433166558414073,
 "sp_prec": 0.6523998114097124,
 "sp_recall": 0.6776190476190473,
 "joint_em": 0.30297029702970296,
 "joint_f1": 0.5295008466806749,
 "joint_prec": 0.5348502424012301,
 "joint_recall": 0.571864616522062
}{
 "em": 0.6871287128712872,
 "f1": 0.7844079220853533,
 "prec": 0.7892267185710737,
 "recall": 0.7944477920467495,
 "sp_em": 0.47128712871287126,
 "sp_f1": 0.6257147857642898,
 "sp_prec": 0.7014521452145214,
 "sp_recall": 0.5963979255068366,
 "joint_em": 0.3900990099009901,
 "joint_f1": 0.5249791425200174,
 "joint_prec": 0.5820963694924092,
 "joint_recall": 0.5126796011211516
}{
 "em": 0.691089108910891,
 "f1": 0.7814094819449768,
 "prec": 0.7874925421210438,
 "recall": 0.7926001319215499,
 "sp_em": 0.48118811881188117,
 "sp_f1": 0.6510638206677799,
 "sp_prec": 0.7028217821782177,
 "sp_recall": 0.6389886845827438,
 "joint_em": 0.400990099009901,
 "joint_f1": 0.5419423525542145,
 "joint_prec": 0.5820361722093531,
 "joint_recall": 0.5436224137697602
}{
 "em": 0.7069306930693069,
 "f1": 0.7945938759154714,
 "prec": 0.8018220346405144,
 "recall": 0.8042373219859539,
 "sp_em": 0.4762376237623762,
 "sp_f1": 0.6513290614775751,
 "sp_prec": 0.7009900990099005,
 "sp_recall": 0.6417939651107966,
 "joint_em": 0.402970297029703,
 "joint_f1": 0.5459145237254431,
 "joint_prec": 0.5887156305567224,
 "joint_recall": 0.5466284129290331
}{
 "em": 0.696039603960396,
 "f1": 0.7918218774209724,
 "prec": 0.7977830740786467,
 "recall": 0.8052281880508778,
 "sp_em": 0.45742574257425744,
 "sp_f1": 0.6587338184367877,
 "sp_prec": 0.6706891403426052,
 "sp_recall": 0.6821664309288068,
 "joint_em": 0.3910891089108911,
 "joint_f1": 0.5525574896829818,
 "joint_prec": 0.5652464065217678,
 "joint_recall": 0.5838827578275848
}{
 "em": 0.7,
 "f1": 0.7955989894938427,
 "prec": 0.798716826375581,
 "recall": 0.808983589512683,
 "sp_em": 0.4910891089108911,
 "sp_f1": 0.6572416527367007,
 "sp_prec": 0.6935502121640732,
 "sp_recall": 0.6584936350777931,
 "joint_em": 0.4079207920792079,
 "joint_f1": 0.5486868764644852,
 "joint_prec": 0.573873234174757,
 "joint_recall": 0.5620774127186787
}{
 "em": 0.69009900990099,
 "f1": 0.7871169974838633,
 "prec": 0.7929403090203344,
 "recall": 0.8004279662119017,
 "sp_em": 0.4495049504950495,
 "sp_f1": 0.6612902718843299,
 "sp_prec": 0.693778877887788,
 "sp_recall": 0.6644601603017442,
 "joint_em": 0.36534653465346534,
 "joint_f1": 0.5528523160450831,
 "joint_prec": 0.579977524121051,
 "joint_recall": 0.5670980657274896
}{
 "em": 0.7049504950495049,
 "f1": 0.8041850723293785,
 "prec": 0.8077079255840068,
 "recall": 0.8199414024829971,
 "sp_em": 0.4801980198019802,
 "sp_f1": 0.6703421056391344,
 "sp_prec": 0.6937623762376235,
 "sp_recall": 0.6800966525223952,
 "joint_em": 0.402970297029703,
 "joint_f1": 0.5659597742191121,
 "joint_prec": 0.5849243172791875,
 "joint_recall": 0.5875589779246065
}{
 "em": 0.7059405940594059,
 "f1": 0.8086029033979332,
 "prec": 0.8125573258813045,
 "recall": 0.8226661765131952,
 "sp_em": 0.4861386138613861,
 "sp_f1": 0.6617654622605105,
 "sp_prec": 0.6983003300330028,
 "sp_recall": 0.6611268269684111,
 "joint_em": 0.4,
 "joint_f1": 0.558231569514986,
 "joint_prec": 0.5894152690113624,
 "joint_recall": 0.5696928532684058
}{
 "em": 0.7029702970297029,
 "f1": 0.80189726916013,
 "prec": 0.8073721421083659,
 "recall": 0.8149322870528206,
 "sp_em": 0.48118811881188117,
 "sp_f1": 0.6628152815281514,
 "sp_prec": 0.7003748231966049,
 "sp_recall": 0.6622489391796322,
 "joint_em": 0.400990099009901,
 "joint_f1": 0.5593295209376165,
 "joint_prec": 0.5918284988279815,
 "joint_recall": 0.5694677333434095
}
{
  "name": "train_v1",
  "prediction_path": "output/submissions/train_v1",
  "checkpoint_path": "output/checkpoints/train_v1",
  "data_dir": "./data/output",
  "fp16": false,
  "ckpt_id": 0,
  "bert_model": "/data/wuyunzhao/pre-model/chinese_roberta_wwm_large_ext_pytorch",
  "epochs": 10,
  "qat_epochs": 0,
  "batch_size": 2,
  "max_bert_size": 8,
  "eval_batch_size": 2,
  "lr": 1e-05,
  "decay": 1.0,
  "early_stop_epoch": 0,
  "verbose_step": 50,
  "gradient_accumulation_steps": 1,
  "seed": 5,
  "q_update": false,
  "prediction_trans": false,
  "trans_drop": 0.5,
  "trans_heads": 3,
  "input_dim": 768,
  "model_gpu": "2,3",
  "trained_weight": null,
  "type_lambda": 1,
  "sp_lambda": 5,
  "sp_threshold": 0.5,
  "label_type_num": 4,
  "max_query_len": 50,
  "max_doc_len": 512
}
{
 "em": 0.6663366336633664,
 "f1": 0.7616315431571244,
 "prec": 0.7682242453032189,
 "recall": 0.7728836535551816,
 "sp_em": 0.4198019801980198,
 "sp_f1": 0.5938071664309287,
 "sp_prec": 0.7083003300330031,
 "sp_recall": 0.5469401225836875,
 "joint_em": 0.3336633663366337,
 "joint_f1": 0.4853262407283525,
 "joint_prec": 0.5704634222040672,
 "joint_recall": 0.45965950955691853
}{
 "em": 0.6851485148514852,
 "f1": 0.7881934844330106,
 "prec": 0.7925190793609256,
 "recall": 0.8031287650805785,
 "sp_em": 0.46732673267326735,
 "sp_f1": 0.6620302030203006,
 "sp_prec": 0.708335690711928,
 "sp_recall": 0.6575200377180574,
 "joint_em": 0.37524752475247525,
 "joint_f1": 0.5531210463445531,
 "joint_prec": 0.5879157821352636,
 "joint_recall": 0.5644201783640004
}{
 "em": 0.692079207920792,
 "f1": 0.7933872572302055,
 "prec": 0.797568970109201,
 "recall": 0.8095906414350207,
 "sp_em": 0.4900990099009901,
 "sp_f1": 0.6425215378680716,
 "sp_prec": 0.7129702970297026,
 "sp_recall": 0.6162494106553514,
 "joint_em": 0.400990099009901,
 "joint_f1": 0.537365592372584,
 "joint_prec": 0.5908610970777233,
 "joint_recall": 0.5311790337579638
}{
 "em": 0.6841584158415842,
 "f1": 0.7866002285345967,
 "prec": 0.7968929924418949,
 "recall": 0.798182334140892,
 "sp_em": 0.48316831683168315,
 "sp_f1": 0.6185187804494726,
 "sp_prec": 0.7062871287128711,
 "sp_recall": 0.5842032060348896,
 "joint_em": 0.3910891089108911,
 "joint_f1": 0.5236352738615434,
 "joint_prec": 0.6007278465301168,
 "joint_recall": 0.5060679016257583
}{
 "em": 0.697029702970297,
 "f1": 0.8066373178790653,
 "prec": 0.8103515781726306,
 "recall": 0.8229831860503483,
 "sp_em": 0.49603960396039604,
 "sp_f1": 0.6699067763919239,
 "sp_prec": 0.7112101210121012,
 "sp_recall": 0.663048090523338,
 "joint_em": 0.39801980198019804,
 "joint_f1": 0.568655297844511,
 "joint_prec": 0.6027935187328965,
 "joint_recall": 0.5764444339826724
}{
 "em": 0.7069306930693069,
 "f1": 0.8088954424075447,
 "prec": 0.816792982323947,
 "recall": 0.8239172040746473,
 "sp_em": 0.4910891089108911,
 "sp_f1": 0.6654916920263443,
 "sp_prec": 0.7115071507150713,
 "sp_recall": 0.6545261669024044,
 "joint_em": 0.39801980198019804,
 "joint_f1": 0.5646947053957523,
 "joint_prec": 0.6078278449389516,
 "joint_recall": 0.5670371411817503
}{
 "em": 0.6881188118811881,
 "f1": 0.7956750960285875,
 "prec": 0.8036318858411459,
 "recall": 0.8143855402536541,
 "sp_em": 0.47128712871287126,
 "sp_f1": 0.6764594316574504,
 "sp_prec": 0.7044295143800088,
 "sp_recall": 0.6805327675624704,
 "joint_em": 0.3792079207920792,
 "joint_f1": 0.5702318658368664,
 "joint_prec": 0.594856554978421,
 "joint_recall": 0.5894111826054688
}{
 "em": 0.7108910891089109,
 "f1": 0.815774390263452,
 "prec": 0.8220353141653811,
 "recall": 0.834567263440007,
 "sp_em": 0.4782178217821782,
 "sp_f1": 0.6724208849456366,
 "sp_prec": 0.694427942794279,
 "sp_recall": 0.67995756718529,
 "joint_em": 0.39504950495049507,
 "joint_f1": 0.575051674672364,
 "joint_prec": 0.5961900628888087,
 "joint_recall": 0.5988046952591072
}{
 "em": 0.7029702970297029,
 "f1": 0.8062657490775932,
 "prec": 0.8123740129707979,
 "recall": 0.8225274016811033,
 "sp_em": 0.48514851485148514,
 "sp_f1": 0.6716043747231856,
 "sp_prec": 0.7007197862643405,
 "sp_recall": 0.6730080150872229,
 "joint_em": 0.3940594059405941,
 "joint_f1": 0.5681091353493645,
 "joint_prec": 0.5971440413849253,
 "joint_recall": 0.584242513448763
}{
 "em": 0.700990099009901,
 "f1": 0.8073100884685173,
 "prec": 0.813759632900748,
 "recall": 0.8231102919979469,
 "sp_em": 0.48514851485148514,
 "sp_f1": 0.672179646536081,
 "sp_prec": 0.7019503378909314,
 "sp_recall": 0.6723314474304573,
 "joint_em": 0.39900990099009903,
 "joint_f1": 0.5700681653325809,
 "joint_prec": 0.5977166259741777,
 "joint_recall": 0.5845755889290506
}{
  "name": "train_v1",
  "prediction_path": "output/submissions/train_v1",
  "checkpoint_path": "output/checkpoints/train_v1",
  "data_dir": "./data/output",
  "fp16": false,
  "ckpt_id": 0,
  "bert_model": "/data/wuyunzhao/pre-model/chinese_roberta_wwm_large_ext_pytorch",
  "epochs": 8,
  "qat_epochs": 0,
  "batch_size": 2,
  "max_bert_size": 8,
  "eval_batch_size": 2,
  "lr": 1e-05,
  "decay": 1.0,
  "early_stop_epoch": 0,
  "verbose_step": 50,
  "gradient_accumulation_steps": 1,
  "seed": 5,
  "q_update": false,
  "prediction_trans": false,
  "trans_drop": 0.5,
  "trans_heads": 3,
  "input_dim": 768,
  "model_gpu": "0,2",
  "trained_weight": null,
  "type_lambda": 1,
  "sp_lambda": 5,
  "sp_threshold": 0.5,
  "label_type_num": 4,
  "max_query_len": 50,
  "max_doc_len": 512
}{
 "em": 0.6683168316831684,
 "f1": 0.7554883701853914,
 "prec": 0.7637217546689512,
 "recall": 0.7663052914620678,
 "sp_em": 0.4178217821782178,
 "sp_f1": 0.6124089551812315,
 "sp_prec": 0.6861173974540308,
 "sp_recall": 0.5925271098538426,
 "joint_em": 0.33762376237623765,
 "joint_f1": 0.491353659408757,
 "joint_prec": 0.5441643491647451,
 "joint_recall": 0.4893114378680012
}{
 "em": 0.7069306930693069,
 "f1": 0.8049899566820365,
 "prec": 0.8062702017035631,
 "recall": 0.8210703192698966,
 "sp_em": 0.46633663366336636,
 "sp_f1": 0.634997571185689,
 "sp_prec": 0.7150165016501647,
 "sp_recall": 0.6047147571900049,
 "joint_em": 0.3831683168316832,
 "joint_f1": 0.5450752748700374,
 "joint_prec": 0.6083255712044562,
 "joint_recall": 0.5339958740680626
}{
 "em": 0.69009900990099,
 "f1": 0.7934257881265254,
 "prec": 0.7976174903213942,
 "recall": 0.8077393195114714,
 "sp_em": 0.49603960396039604,
 "sp_f1": 0.6440522623690929,
 "sp_prec": 0.7218481848184817,
 "sp_recall": 0.6130975954738331,
 "joint_em": 0.3920792079207921,
 "joint_f1": 0.539061374893154,
 "joint_prec": 0.6013192363419158,
 "joint_recall": 0.5274826453806343
}{
 "em": 0.6851485148514852,
 "f1": 0.7954985482420618,
 "prec": 0.8000028841173313,
 "recall": 0.8104364602670462,
 "sp_em": 0.46831683168316834,
 "sp_f1": 0.6633954824053823,
 "sp_prec": 0.7041584158415838,
 "sp_recall": 0.6589250353606788,
 "joint_em": 0.3782178217821782,
 "joint_f1": 0.557008767933699,
 "joint_prec": 0.5867345606095848,
 "joint_recall": 0.5684668023478853
}{
 "em": 0.7,
 "f1": 0.7987481724973764,
 "prec": 0.8041090207521986,
 "recall": 0.8135150955954418,
 "sp_em": 0.47029702970297027,
 "sp_f1": 0.664375223236608,
 "sp_prec": 0.7096369636963694,
 "sp_recall": 0.6549552098066949,
 "joint_em": 0.3891089108910891,
 "joint_f1": 0.5588225794372252,
 "joint_prec": 0.5950620706047584,
 "joint_recall": 0.5646838529600581
}{
 "em": 0.7019801980198019,
 "f1": 0.8042320461249503,
 "prec": 0.8100390314472136,
 "recall": 0.8188975380421181,
 "sp_em": 0.4861386138613861,
 "sp_f1": 0.670129655822724,
 "sp_prec": 0.7061409712399808,
 "sp_recall": 0.6681966053748235,
 "joint_em": 0.39801980198019804,
 "joint_f1": 0.5694195818236144,
 "joint_prec": 0.5992409230877278,
 "joint_recall": 0.5805575634996104
}{
 "em": 0.7039603960396039,
 "f1": 0.8110068727988609,
 "prec": 0.8152813375634609,
 "recall": 0.825907801010133,
 "sp_em": 0.48217821782178216,
 "sp_f1": 0.6671893617933209,
 "sp_prec": 0.6980080150872229,
 "sp_recall": 0.6691041961338992,
 "joint_em": 0.3910891089108911,
 "joint_f1": 0.5719874091211108,
 "joint_prec": 0.597357850206079,
 "joint_recall": 0.5867725018701984
}{
 "em": 0.7089108910891089,
 "f1": 0.8114980694706773,
 "prec": 0.8175047586851453,
 "recall": 0.8234836300216056,
 "sp_em": 0.49405940594059405,
 "sp_f1": 0.6705762719129041,
 "sp_prec": 0.7114521452145212,
 "sp_recall": 0.6630480905233382,
 "joint_em": 0.4,
 "joint_f1": 0.5747111130401861,
 "joint_prec": 0.6101547492761056,
 "joint_recall": 0.5795693857252506
}{
  "name": "train_v1",
  "prediction_path": "output/submissions/train_v1",
  "checkpoint_path": "output/checkpoints/train_v1",
  "data_dir": "./data/output",
  "fp16": false,
  "ckpt_id": 0,
  "bert_model": "/data/wuyunzhao/pre-model/chinese_roberta_wwm_large_ext_pytorch",
  "epochs": 8,
  "qat_epochs": 0,
  "batch_size": 2,
  "max_bert_size": 8,
  "eval_batch_size": 2,
  "lr": 1.5e-05,
  "decay": 1.0,
  "early_stop_epoch": 0,
  "verbose_step": 50,
  "gradient_accumulation_steps": 1,
  "seed": 5,
  "q_update": false,
  "prediction_trans": false,
  "trans_drop": 0.5,
  "trans_heads": 3,
  "input_dim": 768,
  "model_gpu": "0,1",
  "trained_weight": null,
  "type_lambda": 1,
  "sp_lambda": 5,
  "sp_threshold": 0.5,
  "label_type_num": 4,
  "max_query_len": 50,
  "max_doc_len": 512
}{
 "em": 0.6485148514851485,
 "f1": 0.7404870354169419,
 "prec": 0.7455703583500486,
 "recall": 0.7539304361582733,
 "sp_em": 0.42475247524752474,
 "sp_f1": 0.5948813452773845,
 "sp_prec": 0.70019801980198,
 "sp_recall": 0.5535478547854786,
 "joint_em": 0.33762376237623765,
 "joint_f1": 0.4710542798656136,
 "joint_prec": 0.5455138813677642,
 "joint_recall": 0.4543820576552673
}{
 "em": 0.6207920792079208,
 "f1": 0.7145696524010671,
 "prec": 0.7181056928910281,
 "recall": 0.7282704896443585,
 "sp_em": 0.4643564356435644,
 "sp_f1": 0.6028354978354968,
 "sp_prec": 0.671270627062706,
 "sp_recall": 0.5780975954738337,
 "joint_em": 0.3702970297029703,
 "joint_f1": 0.48100300576238625,
 "joint_prec": 0.5235953089551656,
 "joint_recall": 0.4773572899963054
}{
 "em": 0.6732673267326733,
 "f1": 0.7901531040051963,
 "prec": 0.7919622189759886,
 "recall": 0.8123516901852699,
 "sp_em": 0.4752475247524752,
 "sp_f1": 0.6358030088723143,
 "sp_prec": 0.7086468646864684,
 "sp_recall": 0.6098396982555399,
 "joint_em": 0.38415841584158417,
 "joint_f1": 0.5353682671961266,
 "joint_prec": 0.5904999262134103,
 "joint_recall": 0.5316444788419916
}{
 "em": 0.6861386138613862,
 "f1": 0.798938898980131,
 "prec": 0.8041696837843536,
 "recall": 0.8189448488156398,
 "sp_em": 0.4762376237623762,
 "sp_f1": 0.6431639592530668,
 "sp_prec": 0.7181848184818481,
 "sp_recall": 0.6149622819424796,
 "joint_em": 0.3712871287128713,
 "joint_f1": 0.5449038152589368,
 "joint_prec": 0.6039811133961691,
 "joint_recall": 0.5400169501201713
}{
 "em": 0.691089108910891,
 "f1": 0.7933471178925869,
 "prec": 0.8004804358357054,
 "recall": 0.8069576199766864,
 "sp_em": 0.47227722772277225,
 "sp_f1": 0.6557332876144748,
 "sp_prec": 0.7095780292314945,
 "sp_recall": 0.6401107967939649,
 "joint_em": 0.3702970297029703,
 "joint_f1": 0.5516969318233961,
 "joint_prec": 0.6002741945145913,
 "joint_recall": 0.5520739967912517
}{
 "em": 0.6772277227722773,
 "f1": 0.7790538735612845,
 "prec": 0.7874113714800778,
 "recall": 0.797170438839004,
 "sp_em": 0.4495049504950495,
 "sp_f1": 0.6742749439779132,
 "sp_prec": 0.6846829325789715,
 "sp_recall": 0.6946086751532291,
 "joint_em": 0.3564356435643564,
 "joint_f1": 0.55060271642949,
 "joint_prec": 0.5658884476253534,
 "joint_recall": 0.5815552355211405
}{
 "em": 0.696039603960396,
 "f1": 0.7978292202462375,
 "prec": 0.8039172638825173,
 "recall": 0.8148444022417998,
 "sp_em": 0.47425742574257423,
 "sp_f1": 0.6698296972554388,
 "sp_prec": 0.6895568128241388,
 "sp_recall": 0.6821499292786417,
 "joint_em": 0.38613861386138615,
 "joint_f1": 0.5590164963233453,
 "joint_prec": 0.5778308802074383,
 "joint_recall": 0.5839112448399628
}{
 "em": 0.693069306930693,
 "f1": 0.798386054439914,
 "prec": 0.805022777637983,
 "recall": 0.8140199315458824,
 "sp_em": 0.46732673267326735,
 "sp_f1": 0.6700441472718693,
 "sp_prec": 0.6941159830268736,
 "sp_recall": 0.6779915134370574,
 "joint_em": 0.37425742574257426,
 "joint_f1": 0.5592276678198231,
 "joint_prec": 0.5832775813021295,
 "joint_recall": 0.5800680966800501
}{
  "name": "train_v1",
  "prediction_path": "output/submissions/train_v1",
  "checkpoint_path": "output/checkpoints/train_v1",
  "data_dir": "./data/output",
  "fp16": false,
  "ckpt_id": 0,
  "bert_model": "/data/wuyunzhao/pre-model/chinese_roberta_wwm_large_ext_pytorch",
  "epochs": 10,
  "qat_epochs": 0,
  "batch_size": 2,
  "max_bert_size": 8,
  "eval_batch_size": 2,
  "lr": 8e-06,
  "decay": 1.0,
  "early_stop_epoch": 0,
  "verbose_step": 50,
  "gradient_accumulation_steps": 1,
  "seed": 0,
  "q_update": false,
  "prediction_trans": false,
  "trans_drop": 0.5,
  "trans_heads": 3,
  "input_dim": 768,
  "model_gpu": "0,1/",
  "trained_weight": null,
  "type_lambda": 1,
  "sp_lambda": 5,
  "sp_threshold": 0.5,
  "label_type_num": 4,
  "max_query_len": 64,
  "max_doc_len": 512
}{
 "em": 0.6881188118811881,
 "f1": 0.7776743848244869,
 "prec": 0.7825262232227321,
 "recall": 0.7919773071536775,
 "sp_em": 0.4099009900990099,
 "sp_f1": 0.628980040861228,
 "sp_prec": 0.6907873644507302,
 "sp_recall": 0.6193752946723243,
 "joint_em": 0.33663366336633666,
 "joint_f1": 0.5199625412153323,
 "joint_prec": 0.565883156333087,
 "joint_recall": 0.526634082571212
}{
 "em": 0.6732673267326733,
 "f1": 0.7664140440079451,
 "prec": 0.7721490125183478,
 "recall": 0.7818319213036287,
 "sp_em": 0.45643564356435645,
 "sp_f1": 0.6600457902933137,
 "sp_prec": 0.7034464875058926,
 "sp_recall": 0.6555563413484203,
 "joint_em": 0.36237623762376237,
 "joint_f1": 0.5339112808482326,
 "joint_prec": 0.5698453854233271,
 "joint_recall": 0.5434886055809667
}{
 "em": 0.6712871287128713,
 "f1": 0.7813693304618473,
 "prec": 0.7829765205775516,
 "recall": 0.8015099515634351,
 "sp_em": 0.4772277227722772,
 "sp_f1": 0.6553544090672786,
 "sp_prec": 0.7065040075436113,
 "sp_recall": 0.6436020744931632,
 "joint_em": 0.3801980198019802,
 "joint_f1": 0.5407463965963794,
 "joint_prec": 0.5767948198598547,
 "joint_recall": 0.5498442572583909
}{
 "em": 0.696039603960396,
 "f1": 0.7978736888795952,
 "prec": 0.8017339031405162,
 "recall": 0.8149883699862107,
 "sp_em": 0.499009900990099,
 "sp_f1": 0.656344205849155,
 "sp_prec": 0.7092715700141441,
 "sp_recall": 0.6413248467703909,
 "joint_em": 0.4069306930693069,
 "joint_f1": 0.5541823794750239,
 "joint_prec": 0.5967928131690236,
 "joint_recall": 0.5563066487017446
}{
 "em": 0.696039603960396,
 "f1": 0.7965799480978722,
 "prec": 0.8025358659628578,
 "recall": 0.8117981897071596,
 "sp_em": 0.4782178217821782,
 "sp_f1": 0.6668134670609907,
 "sp_prec": 0.7142126355492691,
 "sp_recall": 0.6581565299387077,
 "joint_em": 0.39603960396039606,
 "joint_f1": 0.5615758305512843,
 "joint_prec": 0.5998200118912782,
 "joint_recall": 0.5691254289017591
}{
 "em": 0.698019801980198,
 "f1": 0.7911796434169382,
 "prec": 0.7988392698016595,
 "recall": 0.805691561882326,
 "sp_em": 0.47128712871287126,
 "sp_f1": 0.6708305116225897,
 "sp_prec": 0.6980520194876627,
 "sp_recall": 0.6772324375294668,
 "joint_em": 0.3920792079207921,
 "joint_f1": 0.5603421767960682,
 "joint_prec": 0.5847891405161744,
 "joint_recall": 0.5789851408283528
}{
 "em": 0.6881188118811881,
 "f1": 0.7927390949474318,
 "prec": 0.797688135564425,
 "recall": 0.8079412061698057,
 "sp_em": 0.48316831683168315,
 "sp_f1": 0.6658333690511897,
 "sp_prec": 0.7145120226308342,
 "sp_recall": 0.654271570014144,
 "joint_em": 0.3930693069306931,
 "joint_f1": 0.5578216009141779,
 "joint_prec": 0.597616657902486,
 "joint_recall": 0.5633574927802414
}{
 "em": 0.695049504950495,
 "f1": 0.7915188240378669,
 "prec": 0.7942521753103327,
 "recall": 0.8081798763006722,
 "sp_em": 0.4693069306930693,
 "sp_f1": 0.6550087151572287,
 "sp_prec": 0.698717586044318,
 "sp_recall": 0.6473974540311168,
 "joint_em": 0.38811881188118813,
 "joint_f1": 0.555256571292196,
 "joint_prec": 0.5914431327958819,
 "joint_recall": 0.563503754880517
}{
 "em": 0.696039603960396,
 "f1": 0.7905796555145964,
 "prec": 0.797537417223652,
 "recall": 0.8046526786663806,
 "sp_em": 0.47425742574257423,
 "sp_f1": 0.6670021452694708,
 "sp_prec": 0.7091136256482787,
 "sp_recall": 0.6616643092880711,
 "joint_em": 0.39603960396039606,
 "joint_f1": 0.5589845242914369,
 "joint_prec": 0.5953636173723267,
 "joint_recall": 0.5672071632247391
}{
 "em": 0.692079207920792,
 "f1": 0.7882457064567665,
 "prec": 0.7932007508547301,
 "recall": 0.8054829759818264,
 "sp_em": 0.48217821782178216,
 "sp_f1": 0.6661379159894,
 "sp_prec": 0.7093729372937292,
 "sp_recall": 0.6586350777934933,
 "joint_em": 0.400990099009901,
 "joint_f1": 0.5562776243777757,
 "joint_prec": 0.5911157131162565,
 "joint_recall": 0.5648228167314822
}{
  "name": "train_v1",
  "prediction_path": "output/submissions/train_v1",
  "checkpoint_path": "output/checkpoints/train_v1",
  "data_dir": "./data/output",
  "fp16": false,
  "ckpt_id": 0,
  "bert_model": "/data/wuyunzhao/pre-model/chinese_roberta_wwm_large_ext_pytorch",
  "epochs": 10,
  "qat_epochs": 0,
  "batch_size": 2,
  "max_bert_size": 8,
  "eval_batch_size": 2,
  "lr": 1e-05,
  "decay": 1.0,
  "early_stop_epoch": 0,
  "verbose_step": 50,
  "gradient_accumulation_steps": 1,
  "seed": 0,
  "q_update": false,
  "prediction_trans": false,
  "trans_drop": 0.5,
  "trans_heads": 3,
  "input_dim": 768,
  "model_gpu": "0,1/",
  "trained_weight": null,
  "type_lambda": 1,
  "sp_lambda": 5,
  "sp_threshold": 0.5,
  "label_type_num": 4,
  "max_query_len": 64,
  "max_doc_len": 512
}{
 "em": 0.6594059405940594,
 "f1": 0.7626461160129452,
 "prec": 0.7648968397697553,
 "recall": 0.781255215199175,
 "sp_em": 0.4198019801980198,
 "sp_f1": 0.6423018730444455,
 "sp_prec": 0.7011103253182458,
 "sp_recall": 0.6327510608203675,
 "joint_em": 0.32574257425742575,
 "joint_f1": 0.5129052188885957,
 "joint_prec": 0.5523255206298807,
 "joint_recall": 0.5232071032384916
}{
 "em": 0.694059405940594,
 "f1": 0.7899795334819721,
 "prec": 0.7958022772574515,
 "recall": 0.8021857525933632,
 "sp_em": 0.48118811881188117,
 "sp_f1": 0.6388192390667626,
 "sp_prec": 0.7024587458745872,
 "sp_recall": 0.6203088165959455,
 "joint_em": 0.3910891089108911,
 "joint_f1": 0.5376932351109344,
 "joint_prec": 0.5895807558225425,
 "joint_recall": 0.5320023643671564
}{
 "em": 0.7069306930693069,
 "f1": 0.8022086411124179,
 "prec": 0.8065209352084468,
 "recall": 0.8166907047874876,
 "sp_em": 0.4900990099009901,
 "sp_f1": 0.6543695083794083,
 "sp_prec": 0.7085643564356431,
 "sp_recall": 0.6377180575200377,
 "joint_em": 0.3930693069306931,
 "joint_f1": 0.5499431877863369,
 "joint_prec": 0.5938975869853256,
 "joint_recall": 0.5509134708603906
}{
 "em": 0.700990099009901,
 "f1": 0.7987162425501281,
 "prec": 0.8037952729770764,
 "recall": 0.8106812616485315,
 "sp_em": 0.4801980198019802,
 "sp_f1": 0.6612391239123899,
 "sp_prec": 0.7076355492692122,
 "sp_recall": 0.6525035360678925,
 "joint_em": 0.38415841584158417,
 "joint_f1": 0.5549987216671614,
 "joint_prec": 0.5942276326313393,
 "joint_recall": 0.5579989770542829
}{
 "em": 0.7079207920792079,
 "f1": 0.8052339412997369,
 "prec": 0.8092064360961929,
 "recall": 0.8220095886311203,
 "sp_em": 0.49207920792079207,
 "sp_f1": 0.6591134113411331,
 "sp_prec": 0.7086162187647334,
 "sp_recall": 0.6465959453088166,
 "joint_em": 0.39504950495049507,
 "joint_f1": 0.5562259841923015,
 "joint_prec": 0.5946501178849147,
 "joint_recall": 0.562957472529304
}{
 "em": 0.691089108910891,
 "f1": 0.794731618971584,
 "prec": 0.7983599240816208,
 "recall": 0.8109668676643857,
 "sp_em": 0.4910891089108911,
 "sp_f1": 0.6583799094195123,
 "sp_prec": 0.6955917020273449,
 "sp_recall": 0.657100424328147,
 "joint_em": 0.38712871287128714,
 "joint_f1": 0.5481836050674928,
 "joint_prec": 0.5773907316196956,
 "joint_recall": 0.5626822403870054
}{
 "em": 0.692079207920792,
 "f1": 0.7895656098085148,
 "prec": 0.7953721124173894,
 "recall": 0.8021370479860127,
 "sp_em": 0.47227722772277225,
 "sp_f1": 0.6646236052176637,
 "sp_prec": 0.7006129184347004,
 "sp_recall": 0.6625035360678925,
 "joint_em": 0.38514851485148516,
 "joint_f1": 0.5569351322429742,
 "joint_prec": 0.5874132474917594,
 "joint_recall": 0.5666135785004955
}{
 "em": 0.7029702970297029,
 "f1": 0.8004639834083522,
 "prec": 0.8070968016400273,
 "recall": 0.8120406350257717,
 "sp_em": 0.49207920792079207,
 "sp_f1": 0.6676656951409417,
 "sp_prec": 0.712312588401697,
 "sp_recall": 0.6590381895332392,
 "joint_em": 0.401980198019802,
 "joint_f1": 0.5641411499050472,
 "joint_prec": 0.6033477564982818,
 "joint_recall": 0.5675364681449274
}{
 "em": 0.692079207920792,
 "f1": 0.7966251987832269,
 "prec": 0.7991580432649431,
 "recall": 0.8115796194220785,
 "sp_em": 0.4792079207920792,
 "sp_f1": 0.6689780406612083,
 "sp_prec": 0.6961727958510133,
 "sp_recall": 0.6744271570014145,
 "joint_em": 0.3900990099009901,
 "joint_f1": 0.5603514041497174,
 "joint_prec": 0.5820552238177262,
 "joint_recall": 0.5778860724594787
}{
 "em": 0.689108910891089,
 "f1": 0.7941219722156537,
 "prec": 0.7977203240641082,
 "recall": 0.8082877819965776,
 "sp_em": 0.48217821782178216,
 "sp_f1": 0.6707128569999848,
 "sp_prec": 0.7016053748231963,
 "sp_recall": 0.6722489391796324,
 "joint_em": 0.3910891089108911,
 "joint_f1": 0.561888806289499,
 "joint_prec": 0.5885923646960827,
 "joint_recall": 0.5743017667115154
}{
  "name": "train_k1",
  "prediction_path": "output/submissions/train_k1",
  "checkpoint_path": "output/checkpoints/train_k1",
  "data_dir": "./data/output",
  "fp16": false,
  "ckpt_id": 0,
  "bert_model": "/data/wuyunzhao/pre-model/chinese_roberta_wwm_large_ext_pytorch",
  "epochs": 10,
  "qat_epochs": 0,
  "batch_size": 2,
  "max_bert_size": 8,
  "eval_batch_size": 2,
  "lr": 1e-05,
  "decay": 1.0,
  "early_stop_epoch": 0,
  "verbose_step": 50,
  "gradient_accumulation_steps": 1,
  "seed": 5,
  "q_update": false,
  "prediction_trans": false,
  "trans_drop": 0.5,
  "trans_heads": 3,
  "input_dim": 768,
  "model_gpu": "0,1",
  "trained_weight": null,
  "type_lambda": 1,
  "sp_lambda": 5,
  "sp_threshold": 0.5,
  "label_type_num": 4,
  "max_query_len": 64,
  "max_doc_len": 512
}{
 "em": 0.6851485148514852,
 "f1": 0.7669520476445476,
 "prec": 0.7728433832305102,
 "recall": 0.778599464442841,
 "sp_em": 0.400990099009901,
 "sp_f1": 0.5863072900696658,
 "sp_prec": 0.668201320132013,
 "sp_recall": 0.5606753889674683,
 "joint_em": 0.3316831683168317,
 "joint_f1": 0.48043259641303543,
 "joint_prec": 0.543861192883702,
 "joint_recall": 0.47068393661731184
}{
 "em": 0.7039603960396039,
 "f1": 0.7930945724278512,
 "prec": 0.7956187635712642,
 "recall": 0.8086774969960725,
 "sp_em": 0.4801980198019802,
 "sp_f1": 0.6457520422371896,
 "sp_prec": 0.6928547854785477,
 "sp_recall": 0.6382024988213109,
 "joint_em": 0.403960396039604,
 "joint_f1": 0.534938121718647,
 "joint_prec": 0.5694228268606474,
 "joint_recall": 0.5440317537309151
}{
 "em": 0.7059405940594059,
 "f1": 0.7909186176136531,
 "prec": 0.7959166284622542,
 "recall": 0.8054188559690626,
 "sp_em": 0.4643564356435644,
 "sp_f1": 0.6315983026874108,
 "sp_prec": 0.686287128712871,
 "sp_recall": 0.6199092409240926,
 "joint_em": 0.3930693069306931,
 "joint_f1": 0.5318617234447095,
 "joint_prec": 0.5739180121605613,
 "joint_recall": 0.534989455954772
}{
 "em": 0.7178217821782178,
 "f1": 0.8049953295639223,
 "prec": 0.8120000932847358,
 "recall": 0.8189702624590369,
 "sp_em": 0.5,
 "sp_f1": 0.6661630338858047,
 "sp_prec": 0.7149575671852896,
 "sp_recall": 0.6539191419141912,
 "joint_em": 0.4178217821782178,
 "joint_f1": 0.5589623004040993,
 "joint_prec": 0.5971472461609638,
 "joint_recall": 0.5605989506034585
}{
 "em": 0.7188118811881188,
 "f1": 0.8038929897540404,
 "prec": 0.8073831434189738,
 "recall": 0.8204472829641818,
 "sp_em": 0.4772277227722772,
 "sp_f1": 0.6601923049447787,
 "sp_prec": 0.6977094137985221,
 "sp_recall": 0.6615028288543137,
 "joint_em": 0.4079207920792079,
 "joint_f1": 0.5531679509988058,
 "joint_prec": 0.5781501648458781,
 "joint_recall": 0.5689101959907187
}{
 "em": 0.7237623762376237,
 "f1": 0.8174219498003852,
 "prec": 0.820621626096016,
 "recall": 0.8375726117004331,
 "sp_em": 0.4613861386138614,
 "sp_f1": 0.6567119569099754,
 "sp_prec": 0.6891749174917491,
 "sp_recall": 0.660691890617633,
 "joint_em": 0.38712871287128714,
 "joint_f1": 0.5594932442269046,
 "joint_prec": 0.583344096382335,
 "joint_recall": 0.5788751480571195
}{
 "em": 0.7237623762376237,
 "f1": 0.8139203013559607,
 "prec": 0.8234078673104033,
 "recall": 0.8298126882094295,
 "sp_em": 0.4871287128712871,
 "sp_f1": 0.6604486437654743,
 "sp_prec": 0.7128760018859027,
 "sp_recall": 0.6471204620462047,
 "joint_em": 0.41089108910891087,
 "joint_f1": 0.5603924660107139,
 "joint_prec": 0.604758435990625,
 "joint_recall": 0.56330421481161
}{
 "em": 0.7138613861386138,
 "f1": 0.8055212562197687,
 "prec": 0.8104207742097195,
 "recall": 0.8247008823583447,
 "sp_em": 0.48118811881188117,
 "sp_f1": 0.6677795307003213,
 "sp_prec": 0.7052781706742101,
 "sp_recall": 0.6660231023102311,
 "joint_em": 0.402970297029703,
 "joint_f1": 0.5619636608840406,
 "joint_prec": 0.5900034495502241,
 "joint_recall": 0.5778320507442523
}{
 "em": 0.7217821782178218,
 "f1": 0.8096835301053803,
 "prec": 0.8172107930159606,
 "recall": 0.8239011196089603,
 "sp_em": 0.48217821782178216,
 "sp_f1": 0.661259186358195,
 "sp_prec": 0.7030563413484202,
 "sp_recall": 0.6557001414427156,
 "joint_em": 0.4069306930693069,
 "joint_f1": 0.5603660892042087,
 "joint_prec": 0.5965825015275851,
 "joint_recall": 0.5683097014504992
}{
 "em": 0.7257425742574257,
 "f1": 0.8136442397088425,
 "prec": 0.8219666297042499,
 "recall": 0.8278402992412095,
 "sp_em": 0.48415841584158414,
 "sp_f1": 0.6683157601474419,
 "sp_prec": 0.6987843784378437,
 "sp_recall": 0.6721192833569072,
 "joint_em": 0.4158415841584158,
 "joint_f1": 0.5680211796806994,
 "joint_prec": 0.5928169390998269,
 "joint_recall": 0.5854735011638038
}{
  "name": "train_k",
  "prediction_path": "output/submissions/train_k",
  "checkpoint_path": "output/checkpoints/train_k",
  "data_dir": "./data/output",
  "fp16": false,
  "ckpt_id": 0,
  "bert_model": "./chinese_roberta_wwm_large_ext_pytorch",
  "epochs": 10,
  "qat_epochs": 0,
  "batch_size": 2,
  "max_bert_size": 8,
  "eval_batch_size": 2,
  "lr": 1e-05,
  "decay": 1.0,
  "early_stop_epoch": 0,
  "verbose_step": 50,
  "gradient_accumulation_steps": 1,
  "seed": 0,
  "q_update": false,
  "prediction_trans": false,
  "trans_drop": 0.5,
  "trans_heads": 3,
  "input_dim": 768,
  "model_gpu": "0",
  "trained_weight": "./output/checkpoints/train_v1/pred_seed_10_epoch_8_99999.pth",
  "type_lambda": 1,
  "sp_lambda": 5,
  "sp_threshold": 0.5,
  "label_type_num": 4,
  "max_query_len": 64,
  "max_doc_len": 512
}{
  "name": "train_k1",
  "prediction_path": "output/submissions/train_k1",
  "checkpoint_path": "output/checkpoints/train_k1",
  "data_dir": "./data/output",
  "fp16": false,
  "ckpt_id": 0,
  "bert_model": "/data/wuyunzhao/pre-model/chinese_roberta_wwm_large_ext_pytorch",
  "epochs": 10,
  "qat_epochs": 0,
  "batch_size": 2,
  "max_bert_size": 8,
  "eval_batch_size": 2,
  "lr": 1e-05,
  "decay": 1.0,
  "early_stop_epoch": 0,
  "verbose_step": 50,
  "gradient_accumulation_steps": 1,
  "seed": 5,
  "q_update": false,
  "prediction_trans": false,
  "trans_drop": 0.5,
  "trans_heads": 3,
  "input_dim": 768,
  "model_gpu": "0,1",
  "trained_weight": null,
  "type_lambda": 1,
  "sp_lambda": 5,
  "sp_threshold": 0.5,
  "label_type_num": 4,
  "max_query_len": 64,
  "max_doc_len": 512
}{
  "name": "train_k1",
  "prediction_path": "output/submissions/train_k1",
  "checkpoint_path": "output/checkpoints/train_k1",
  "data_dir": "./data/output",
  "fp16": false,
  "ckpt_id": 0,
  "bert_model": "/data/wuyunzhao/pre-model/chinese_roberta_wwm_large_ext_pytorch",
  "epochs": 10,
  "qat_epochs": 0,
  "batch_size": 2,
  "max_bert_size": 8,
  "eval_batch_size": 2,
  "lr": 1e-05,
  "decay": 1.0,
  "early_stop_epoch": 0,
  "verbose_step": 50,
  "gradient_accumulation_steps": 1,
  "seed": 5,
  "q_update": false,
  "prediction_trans": false,
  "trans_drop": 0.5,
  "trans_heads": 3,
  "input_dim": 768,
  "model_gpu": "0,1",
  "trained_weight": null,
  "type_lambda": 1,
  "sp_lambda": 5,
  "sp_threshold": 0.5,
  "label_type_num": 4,
  "max_query_len": 64,
  "max_doc_len": 512
}{
  "name": "train_k1",
  "prediction_path": "output/submissions/train_k1",
  "checkpoint_path": "output/checkpoints/train_k1",
  "data_dir": "./data/output",
  "fp16": false,
  "ckpt_id": 0,
  "bert_model": "/data/wuyunzhao/pre-model/chinese_roberta_wwm_large_ext_pytorch",
  "epochs": 10,
  "qat_epochs": 0,
  "batch_size": 2,
  "max_bert_size": 8,
  "eval_batch_size": 2,
  "lr": 1e-05,
  "decay": 1.0,
  "early_stop_epoch": 0,
  "verbose_step": 50,
  "gradient_accumulation_steps": 1,
  "seed": 5,
  "q_update": false,
  "prediction_trans": false,
  "trans_drop": 0.5,
  "trans_heads": 3,
  "input_dim": 768,
  "model_gpu": "2,3",
  "trained_weight": null,
  "type_lambda": 1,
  "sp_lambda": 5,
  "sp_threshold": 0.5,
  "label_type_num": 4,
  "max_query_len": 64,
  "max_doc_len": 512
}{
 "em": 0.6752475247524753,
 "f1": 0.7562322496873316,
 "prec": 0.7617883570283722,
 "recall": 0.7685966042503406,
 "sp_em": 0.3891089108910891,
 "sp_f1": 0.5984235566413779,
 "sp_prec": 0.6751980198019801,
 "sp_recall": 0.5778630363036306,
 "joint_em": 0.3227722772277228,
 "joint_f1": 0.4773177674810887,
 "joint_prec": 0.5330061171765949,
 "joint_recall": 0.47294213894040926
}{
 "em": 0.7089108910891089,
 "f1": 0.7958555786498012,
 "prec": 0.80213743949406,
 "recall": 0.8063908140533965,
 "sp_em": 0.44752475247524753,
 "sp_f1": 0.6577611662265116,
 "sp_prec": 0.6865181518151814,
 "sp_recall": 0.6671452145214519,
 "joint_em": 0.37623762376237624,
 "joint_f1": 0.5483849992038093,
 "joint_prec": 0.5722847517678721,
 "joint_recall": 0.5645419413464948
}