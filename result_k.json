{
  "name": "train_k1",
  "prediction_path": "output/submissions/train_k1",
  "checkpoint_path": "output/checkpoints/train_k1",
  "data_dir": "./data/output",
  "fp16": false,
  "ckpt_id": 0,
  "bert_model": "/data/wuyunzhao/pre-model/chinese_roberta_wwm_large_ext_pytorch",
  "epochs": 10,
  "qat_epochs": 0,
  "batch_size": 2,
  "max_bert_size": 8,
  "eval_batch_size": 2,
  "lr": 1e-05,
  "decay": 1.0,
  "early_stop_epoch": 0,
  "verbose_step": 50,
  "gradient_accumulation_steps": 1,
  "seed": 5,
  "q_update": false,
  "prediction_trans": false,
  "trans_drop": 0.5,
  "trans_heads": 3,
  "input_dim": 768,
  "model_gpu": "2,3",
  "trained_weight": null,
  "type_lambda": 1,
  "sp_lambda": 5,
  "sp_threshold": 0.5,
  "label_type_num": 4,
  "max_query_len": 64,
  "max_doc_len": 512
}{
 "em": 0.6415841584158416,
 "f1": 0.7238953644714694,
 "prec": 0.7297624672035439,
 "recall": 0.732643883978475,
 "sp_em": 0.3940594059405941,
 "sp_f1": 0.6278864205101821,
 "sp_prec": 0.6720532767562468,
 "sp_recall": 0.6314427157001414,
 "joint_em": 0.31287128712871287,
 "joint_f1": 0.4754203615765178,
 "joint_prec": 0.499860833690351,
 "joint_recall": 0.4916146643898502
}{
 "em": 0.7198019801980198,
 "f1": 0.8116965481104399,
 "prec": 0.8138860782169405,
 "recall": 0.8325186958035343,
 "sp_em": 0.4792079207920792,
 "sp_f1": 0.6527337733773365,
 "sp_prec": 0.7075742574257421,
 "sp_recall": 0.6389191419141915,
 "joint_em": 0.4099009900990099,
 "joint_f1": 0.5576535146622037,
 "joint_prec": 0.5959022126405321,
 "joint_recall": 0.5637669393492206
}{
 "em": 0.7257425742574257,
 "f1": 0.817665822930258,
 "prec": 0.8278480854249,
 "recall": 0.8323667658112545,
 "sp_em": 0.48415841584158414,
 "sp_f1": 0.6469600531481704,
 "sp_prec": 0.711221122112211,
 "sp_recall": 0.6269719471947194,
 "joint_em": 0.4079207920792079,
 "joint_f1": 0.5506981056858914,
 "joint_prec": 0.6087555843288601,
 "joint_recall": 0.5451133541319588
}{
 "em": 0.7128712871287128,
 "f1": 0.8081184466732145,
 "prec": 0.818647749167875,
 "recall": 0.8207734165369948,
 "sp_em": 0.4881188118811881,
 "sp_f1": 0.6586090037575176,
 "sp_prec": 0.7051485148514849,
 "sp_recall": 0.6493399339933991,
 "joint_em": 0.4069306930693069,
 "joint_f1": 0.5542526399442557,
 "joint_prec": 0.5968397081423935,
 "joint_recall": 0.5565048629796298
}{
 "em": 0.7138613861386138,
 "f1": 0.8087649136123621,
 "prec": 0.814580084673742,
 "recall": 0.8251168519321358,
 "sp_em": 0.4871287128712871,
 "sp_f1": 0.6570505621990756,
 "sp_prec": 0.7117574257425738,
 "sp_recall": 0.6425648279113627,
 "joint_em": 0.41089108910891087,
 "joint_f1": 0.5565061625303149,
 "joint_prec": 0.6006735624136388,
 "joint_recall": 0.5586308268258665
}{
 "em": 0.7207920792079208,
 "f1": 0.8106035510993156,
 "prec": 0.8184069428559918,
 "recall": 0.8223803095704167,
 "sp_em": 0.48514851485148514,
 "sp_f1": 0.6668513664553256,
 "sp_prec": 0.7087352663837811,
 "sp_recall": 0.6616760961810467,
 "joint_em": 0.4158415841584158,
 "joint_f1": 0.561319477352757,
 "joint_prec": 0.5944268645085196,
 "joint_recall": 0.5695014058905752
}{
 "em": 0.7198019801980198,
 "f1": 0.8136884897203857,
 "prec": 0.8206458872182071,
 "recall": 0.8290472067570835,
 "sp_em": 0.4801980198019802,
 "sp_f1": 0.6674662466246613,
 "sp_prec": 0.6963118811881185,
 "sp_recall": 0.6731765676567653,
 "joint_em": 0.402970297029703,
 "joint_f1": 0.564055541830899,
 "joint_prec": 0.5896804654731506,
 "joint_recall": 0.5817199259766204
}{
 "em": 0.694059405940594,
 "f1": 0.7941475067001833,
 "prec": 0.8003388358439761,
 "recall": 0.8096640771599333,
 "sp_em": 0.45445544554455447,
 "sp_f1": 0.672569839401521,
 "sp_prec": 0.6937647336162179,
 "sp_recall": 0.6867138142385663,
 "joint_em": 0.36633663366336633,
 "joint_f1": 0.55607882970791,
 "joint_prec": 0.5750675198408528,
 "joint_recall": 0.5821526959262708
}{
 "em": 0.7257425742574257,
 "f1": 0.8224918205378488,
 "prec": 0.8297056184256686,
 "recall": 0.8358779429942765,
 "sp_em": 0.4752475247524752,
 "sp_f1": 0.6627063420627766,
 "sp_prec": 0.695882838283828,
 "sp_recall": 0.6649504950495048,
 "joint_em": 0.4079207920792079,
 "joint_f1": 0.5700910392736627,
 "joint_prec": 0.5983543639986151,
 "joint_recall": 0.5828744694822314
}{
 "em": 0.7237623762376237,
 "f1": 0.8219641214882302,
 "prec": 0.8301101421475804,
 "recall": 0.8330252662171651,
 "sp_em": 0.4772277227722772,
 "sp_f1": 0.662075421827896,
 "sp_prec": 0.7003382838283826,
 "sp_recall": 0.6607425742574257,
 "joint_em": 0.4099009900990099,
 "joint_f1": 0.5680484408448495,
 "joint_prec": 0.6007235627124072,
 "joint_recall": 0.5763959183252287
}{
  "name": "train_k2",
  "prediction_path": "output/submissions/train_k2",
  "checkpoint_path": "output/checkpoints/train_k2",
  "data_dir": "./data/output",
  "fp16": false,
  "ckpt_id": 0,
  "bert_model": "/data/wuyunzhao/pre-model/chinese_roberta_wwm_large_ext_pytorch",
  "epochs": 10,
  "qat_epochs": 0,
  "batch_size": 2,
  "max_bert_size": 8,
  "eval_batch_size": 2,
  "lr": 1e-05,
  "decay": 1.0,
  "early_stop_epoch": 0,
  "verbose_step": 50,
  "gradient_accumulation_steps": 1,
  "seed": 5,
  "q_update": false,
  "prediction_trans": false,
  "trans_drop": 0.5,
  "trans_heads": 3,
  "input_dim": 768,
  "model_gpu": "2,3",
  "trained_weight": null,
  "type_lambda": 1,
  "sp_lambda": 5,
  "sp_threshold": 0.5,
  "label_type_num": 4,
  "max_query_len": 64,
  "max_doc_len": 512
}{
 "em": 0.6821782178217822,
 "f1": 0.7752501700441323,
 "prec": 0.7802160960723274,
 "recall": 0.7900992100916132,
 "sp_em": 0.41287128712871285,
 "sp_f1": 0.6381877473461623,
 "sp_prec": 0.6775660066006601,
 "sp_recall": 0.6420815652993871,
 "joint_em": 0.33762376237623765,
 "joint_f1": 0.5203717845978271,
 "joint_prec": 0.5516434646742566,
 "joint_recall": 0.5354802893873086
}{
 "em": 0.694059405940594,
 "f1": 0.7935208528397161,
 "prec": 0.8026952279675973,
 "recall": 0.8044406602808407,
 "sp_em": 0.48217821782178216,
 "sp_f1": 0.6423127697385114,
 "sp_prec": 0.7075082508250823,
 "sp_recall": 0.6210337105139091,
 "joint_em": 0.400990099009901,
 "joint_f1": 0.5358757910849113,
 "joint_prec": 0.5912247440088252,
 "joint_recall": 0.5277996501242439
}{
 "em": 0.7138613861386138,
 "f1": 0.8025683048126026,
 "prec": 0.8072647352518617,
 "recall": 0.8153599702675532,
 "sp_em": 0.4910891089108911,
 "sp_f1": 0.6448980612346936,
 "sp_prec": 0.6966949552098065,
 "sp_recall": 0.6336409712399812,
 "joint_em": 0.41188118811881186,
 "joint_f1": 0.5359717125182777,
 "joint_prec": 0.5759076497446536,
 "joint_recall": 0.5372900862447951
}{
 "em": 0.696039603960396,
 "f1": 0.7920031725982664,
 "prec": 0.7985612616614565,
 "recall": 0.8060076358157693,
 "sp_em": 0.49405940594059405,
 "sp_f1": 0.6409055081332299,
 "sp_prec": 0.6924917491749175,
 "sp_recall": 0.628461810466761,
 "joint_em": 0.41287128712871285,
 "joint_f1": 0.536281600224731,
 "joint_prec": 0.5776828777841047,
 "joint_recall": 0.5371698622463312
}{
 "em": 0.7029702970297029,
 "f1": 0.7911203191416425,
 "prec": 0.7972018888868595,
 "recall": 0.8061315801126728,
 "sp_em": 0.503960396039604,
 "sp_f1": 0.6386474691425177,
 "sp_prec": 0.6923385195662424,
 "sp_recall": 0.6252345591702031,
 "joint_em": 0.42475247524752474,
 "joint_f1": 0.5312746615325568,
 "joint_prec": 0.5743011032286923,
 "joint_recall": 0.5352574652017207
}{
 "em": 0.7049504950495049,
 "f1": 0.7953402958552815,
 "prec": 0.8030631281806008,
 "recall": 0.8093528245305524,
 "sp_em": 0.497029702970297,
 "sp_f1": 0.6576882578367715,
 "sp_prec": 0.7023762376237619,
 "sp_recall": 0.652445780292315,
 "joint_em": 0.4158415841584158,
 "joint_f1": 0.5430181279442422,
 "joint_prec": 0.5767532789656085,
 "joint_recall": 0.5535072779297864
}{
 "em": 0.7049504950495049,
 "f1": 0.7980580300143508,
 "prec": 0.8078088796290106,
 "recall": 0.8100364603861603,
 "sp_em": 0.4792079207920792,
 "sp_f1": 0.6609732291910496,
 "sp_prec": 0.690440829797265,
 "sp_recall": 0.6660513908533711,
 "joint_em": 0.4,
 "joint_f1": 0.5483580539302912,
 "joint_prec": 0.5753804229660932,
 "joint_recall": 0.5639737564525108
}{
 "em": 0.7079207920792079,
 "f1": 0.7970113536396446,
 "prec": 0.8049826914293824,
 "recall": 0.8107496072605396,
 "sp_em": 0.48415841584158414,
 "sp_f1": 0.6575520299282661,
 "sp_prec": 0.6828995756718527,
 "sp_recall": 0.6661350777934939,
 "joint_em": 0.401980198019802,
 "joint_f1": 0.5457017444278716,
 "joint_prec": 0.5665304701088921,
 "joint_recall": 0.5671596744247788
}{
 "em": 0.7049504950495049,
 "f1": 0.7952390738337107,
 "prec": 0.8030202176752833,
 "recall": 0.8104339782838353,
 "sp_em": 0.501980198019802,
 "sp_f1": 0.6640336066573677,
 "sp_prec": 0.7011551155115507,
 "sp_recall": 0.6604738330975957,
 "joint_em": 0.4207920792079208,
 "joint_f1": 0.5483116213226242,
 "joint_prec": 0.5787000855875671,
 "joint_recall": 0.5604691356425691
}{
 "em": 0.7099009900990099,
 "f1": 0.7998652678113337,
 "prec": 0.8087730437293191,
 "recall": 0.8139918260927212,
 "sp_em": 0.499009900990099,
 "sp_f1": 0.6642496282595279,
 "sp_prec": 0.7002475247524751,
 "sp_recall": 0.6630728429985859,
 "joint_em": 0.4207920792079208,
 "joint_f1": 0.5514524348795459,
 "joint_prec": 0.5816949744566053,
 "joint_recall": 0.5650910613823483
}{
  "name": "train_k3",
  "prediction_path": "output/submissions/train_k3",
  "checkpoint_path": "output/checkpoints/train_k3",
  "data_dir": "./data/output",
  "fp16": false,
  "ckpt_id": 0,
  "bert_model": "/data/wuyunzhao/pre-model/chinese_roberta_wwm_large_ext_pytorch",
  "epochs": 10,
  "qat_epochs": 0,
  "batch_size": 2,
  "max_bert_size": 8,
  "eval_batch_size": 2,
  "lr": 1e-05,
  "decay": 1.0,
  "early_stop_epoch": 0,
  "verbose_step": 50,
  "gradient_accumulation_steps": 1,
  "seed": 5,
  "q_update": false,
  "prediction_trans": false,
  "trans_drop": 0.5,
  "trans_heads": 3,
  "input_dim": 768,
  "model_gpu": "2,3",
  "trained_weight": null,
  "type_lambda": 1,
  "sp_lambda": 5,
  "sp_threshold": 0.5,
  "label_type_num": 4,
  "max_query_len": 64,
  "max_doc_len": 512
}{
 "em": 0.6752475247524753,
 "f1": 0.7695204936749678,
 "prec": 0.7792336300434075,
 "recall": 0.7781623028662733,
 "sp_em": 0.41089108910891087,
 "sp_f1": 0.6052788135956444,
 "sp_prec": 0.6827227722772277,
 "sp_recall": 0.5805445544554457,
 "joint_em": 0.32475247524752476,
 "joint_f1": 0.49479167819887926,
 "joint_prec": 0.5613254671656035,
 "joint_recall": 0.48329419426089587
}{
 "em": 0.7207920792079208,
 "f1": 0.8074617367230438,
 "prec": 0.8147838516521945,
 "recall": 0.8168329901071794,
 "sp_em": 0.42277227722772276,
 "sp_f1": 0.577466603803237,
 "sp_prec": 0.6973102310231023,
 "sp_recall": 0.5271782178217822,
 "joint_em": 0.3495049504950495,
 "joint_f1": 0.49299031622212336,
 "joint_prec": 0.5933539978178562,
 "joint_recall": 0.46043633454285493
}{
 "em": 0.7158415841584158,
 "f1": 0.8028260602279482,
 "prec": 0.8107798683079002,
 "recall": 0.810888148490764,
 "sp_em": 0.4782178217821782,
 "sp_f1": 0.6371479895242259,
 "sp_prec": 0.6885195662423382,
 "sp_recall": 0.6290594059405941,
 "joint_em": 0.39900990099009903,
 "joint_f1": 0.5374160023579634,
 "joint_prec": 0.5800380296682917,
 "joint_recall": 0.5391789765947418
}{
 "em": 0.7227722772277227,
 "f1": 0.8140731052888567,
 "prec": 0.8202141585920201,
 "recall": 0.8265041174762605,
 "sp_em": 0.4633663366336634,
 "sp_f1": 0.6220415283286558,
 "sp_prec": 0.7052062706270628,
 "sp_recall": 0.5902805280528054,
 "joint_em": 0.38514851485148516,
 "joint_f1": 0.5319524679633199,
 "joint_prec": 0.6029474508084391,
 "joint_recall": 0.5149731777452207
}{
 "em": 0.7198019801980198,
 "f1": 0.8139272556554183,
 "prec": 0.8206926096554178,
 "recall": 0.824429005021661,
 "sp_em": 0.4613861386138614,
 "sp_f1": 0.6397386881545286,
 "sp_prec": 0.6979255068363975,
 "sp_recall": 0.6223125884016976,
 "joint_em": 0.3712871287128713,
 "joint_f1": 0.548071436069189,
 "joint_prec": 0.5991820651579016,
 "joint_recall": 0.5414711686169902
}{
 "em": 0.7168316831683168,
 "f1": 0.8129927922892045,
 "prec": 0.8202694918112035,
 "recall": 0.8247356176639888,
 "sp_em": 0.4801980198019802,
 "sp_f1": 0.6508997328304241,
 "sp_prec": 0.6933663366336629,
 "sp_recall": 0.6436657237152285,
 "joint_em": 0.38514851485148516,
 "joint_f1": 0.5481642443058824,
 "joint_prec": 0.5875720767112528,
 "joint_recall": 0.5529234929840224
}{
 "em": 0.7188118811881188,
 "f1": 0.8102826131203097,
 "prec": 0.8197827249645403,
 "recall": 0.8166985662204371,
 "sp_em": 0.4633663366336634,
 "sp_f1": 0.6545903326596381,
 "sp_prec": 0.6919495520980667,
 "sp_recall": 0.6536680810938236,
 "joint_em": 0.3801980198019802,
 "joint_f1": 0.5528737681442578,
 "joint_prec": 0.586366423604055,
 "joint_recall": 0.5601342209665079
}{
 "em": 0.7237623762376237,
 "f1": 0.8140774400197541,
 "prec": 0.8220114900364683,
 "recall": 0.8221385201348569,
 "sp_em": 0.48217821782178216,
 "sp_f1": 0.6528422842284215,
 "sp_prec": 0.7047194719471949,
 "sp_recall": 0.6410584629891558,
 "joint_em": 0.3940594059405941,
 "joint_f1": 0.5570364133268849,
 "joint_prec": 0.6040267896020021,
 "joint_recall": 0.555495038930541
}{
 "em": 0.7178217821782178,
 "f1": 0.8093981502478346,
 "prec": 0.8164359122453342,
 "recall": 0.819198827721258,
 "sp_em": 0.47425742574257423,
 "sp_f1": 0.6568150386467202,
 "sp_prec": 0.6976143328618571,
 "sp_recall": 0.6537812352663835,
 "joint_em": 0.3801980198019802,
 "joint_f1": 0.5560193272150031,
 "joint_prec": 0.5919090949364396,
 "joint_recall": 0.5642948349637396
}{
 "em": 0.7237623762376237,
 "f1": 0.8145467875678508,
 "prec": 0.8220150842984036,
 "recall": 0.8261057430368498,
 "sp_em": 0.4772277227722772,
 "sp_f1": 0.6539396796822526,
 "sp_prec": 0.6955964167845355,
 "sp_recall": 0.6491277699198491,
 "joint_em": 0.38613861386138615,
 "joint_f1": 0.5559019292557142,
 "joint_prec": 0.5921630027859422,
 "joint_recall": 0.5643406333640685
}{
  "name": "train_k4",
  "prediction_path": "output/submissions/train_k4",
  "checkpoint_path": "output/checkpoints/train_k4",
  "data_dir": "./data/output",
  "fp16": false,
  "ckpt_id": 0,
  "bert_model": "/data/wuyunzhao/pre-model/chinese_roberta_wwm_large_ext_pytorch",
  "epochs": 10,
  "qat_epochs": 0,
  "batch_size": 2,
  "max_bert_size": 8,
  "eval_batch_size": 2,
  "lr": 1e-05,
  "decay": 1.0,
  "early_stop_epoch": 0,
  "verbose_step": 50,
  "gradient_accumulation_steps": 1,
  "seed": 5,
  "q_update": false,
  "prediction_trans": false,
  "trans_drop": 0.5,
  "trans_heads": 3,
  "input_dim": 768,
  "model_gpu": "2,3",
  "trained_weight": null,
  "type_lambda": 1,
  "sp_lambda": 5,
  "sp_threshold": 0.5,
  "label_type_num": 4,
  "max_query_len": 64,
  "max_doc_len": 512
}{
 "em": 0.698019801980198,
 "f1": 0.7879908466216607,
 "prec": 0.7944996561703992,
 "recall": 0.7980853028191088,
 "sp_em": 0.41287128712871285,
 "sp_f1": 0.6347606903547486,
 "sp_prec": 0.6795874587458739,
 "sp_recall": 0.6383969825553978,
 "joint_em": 0.3346534653465347,
 "joint_f1": 0.5255717663958316,
 "joint_prec": 0.5601017806656798,
 "joint_recall": 0.5377261625892317
}{
 "em": 0.7277227722772277,
 "f1": 0.8114881184808268,
 "prec": 0.8172371377579873,
 "recall": 0.8234469709190859,
 "sp_em": 0.49306930693069306,
 "sp_f1": 0.6423191604874754,
 "sp_prec": 0.7028052805280522,
 "sp_recall": 0.6228925035360675,
 "joint_em": 0.403960396039604,
 "joint_f1": 0.546648360438266,
 "joint_prec": 0.599677812209323,
 "joint_recall": 0.5386991371428242
}{
 "em": 0.7425742574257426,
 "f1": 0.8310867200167953,
 "prec": 0.8349948250561815,
 "recall": 0.8480617021319535,
 "sp_em": 0.48217821782178216,
 "sp_f1": 0.6575713999971413,
 "sp_prec": 0.6930787364450723,
 "sp_recall": 0.6571098538425268,
 "joint_em": 0.4158415841584158,
 "joint_f1": 0.5674233331391586,
 "joint_prec": 0.5959596850425716,
 "joint_recall": 0.581481558938741
}{
 "em": 0.7445544554455445,
 "f1": 0.8315478179080203,
 "prec": 0.8352863152514141,
 "recall": 0.8434193079877162,
 "sp_em": 0.4871287128712871,
 "sp_f1": 0.6552270227022687,
 "sp_prec": 0.6951461574728893,
 "sp_recall": 0.6494035832154643,
 "joint_em": 0.4099009900990099,
 "joint_f1": 0.5668789212549397,
 "joint_prec": 0.6007357449911845,
 "joint_recall": 0.5706305205455845
}{
 "em": 0.7495049504950495,
 "f1": 0.833362035996602,
 "prec": 0.8419878761139843,
 "recall": 0.8399757244265281,
 "sp_em": 0.49603960396039604,
 "sp_f1": 0.6593299329932981,
 "sp_prec": 0.6847666195190941,
 "sp_recall": 0.6659877416313059,
 "joint_em": 0.4316831683168317,
 "joint_f1": 0.5724127505593005,
 "joint_prec": 0.5969156524364688,
 "joint_recall": 0.5844791660279607
}{
 "em": 0.7336633663366336,
 "f1": 0.8191416536162107,
 "prec": 0.8261251853784876,
 "recall": 0.8280758842921536,
 "sp_em": 0.4792079207920792,
 "sp_f1": 0.6565802294515154,
 "sp_prec": 0.6863979255068359,
 "sp_recall": 0.6598656294200849,
 "joint_em": 0.39702970297029705,
 "joint_f1": 0.5661468946683537,
 "joint_prec": 0.5928262028419595,
 "joint_recall": 0.574498526831133
}{
 "em": 0.7465346534653465,
 "f1": 0.8325826638677818,
 "prec": 0.8407591894315063,
 "recall": 0.8424158403567428,
 "sp_em": 0.49603960396039604,
 "sp_f1": 0.6556054176846239,
 "sp_prec": 0.6823019801980192,
 "sp_recall": 0.6591914191419144,
 "joint_em": 0.4277227722772277,
 "joint_f1": 0.5663476696727143,
 "joint_prec": 0.5940422812021534,
 "joint_recall": 0.5771677244494836
}{
 "em": 0.7346534653465346,
 "f1": 0.81908438885781,
 "prec": 0.8282300853564271,
 "recall": 0.8308149029520783,
 "sp_em": 0.4910891089108911,
 "sp_f1": 0.6579966568085367,
 "sp_prec": 0.6935101367279579,
 "sp_recall": 0.6549481376709099,
 "joint_em": 0.40594059405940597,
 "joint_f1": 0.5632001931318715,
 "joint_prec": 0.5979242429046212,
 "joint_recall": 0.5674671808657132
}{
 "em": 0.7495049504950495,
 "f1": 0.8381955735723534,
 "prec": 0.8470816487305387,
 "recall": 0.8460952484668443,
 "sp_em": 0.4792079207920792,
 "sp_f1": 0.6592815215587478,
 "sp_prec": 0.6810042432814705,
 "sp_recall": 0.6684818481848183,
 "joint_em": 0.403960396039604,
 "joint_f1": 0.57325238485606,
 "joint_prec": 0.5967160064670085,
 "joint_recall": 0.5866806614522282
}{
 "em": 0.7504950495049505,
 "f1": 0.8392218350061842,
 "prec": 0.8483980760261938,
 "recall": 0.8461244502287191,
 "sp_em": 0.4910891089108911,
 "sp_f1": 0.66151486577229,
 "sp_prec": 0.6850141442715691,
 "sp_recall": 0.6677392739273925,
 "joint_em": 0.41287128712871285,
 "joint_f1": 0.5744246459722102,
 "joint_prec": 0.6012209716640726,
 "joint_recall": 0.5836263817890378
}{
  "name": "train_k5",
  "prediction_path": "output/submissions/train_k5",
  "checkpoint_path": "output/checkpoints/train_k5",
  "data_dir": "./data/output",
  "fp16": false,
  "ckpt_id": 0,
  "bert_model": "/data/wuyunzhao/pre-model/chinese_roberta_wwm_large_ext_pytorch",
  "epochs": 10,
  "qat_epochs": 0,
  "batch_size": 2,
  "max_bert_size": 8,
  "eval_batch_size": 2,
  "lr": 1e-05,
  "decay": 1.0,
  "early_stop_epoch": 0,
  "verbose_step": 50,
  "gradient_accumulation_steps": 1,
  "seed": 5,
  "q_update": false,
  "prediction_trans": false,
  "trans_drop": 0.5,
  "trans_heads": 3,
  "input_dim": 768,
  "model_gpu": "2,3",
  "trained_weight": null,
  "type_lambda": 1,
  "sp_lambda": 5,
  "sp_threshold": 0.5,
  "label_type_num": 4,
  "max_query_len": 64,
  "max_doc_len": 512
}{
 "em": 0.6643564356435644,
 "f1": 0.7485973529708488,
 "prec": 0.7573873998246318,
 "recall": 0.7642694168956689,
 "sp_em": 0.4405940594059406,
 "sp_f1": 0.6074654498416865,
 "sp_prec": 0.6872937293729371,
 "sp_recall": 0.5841654879773691,
 "joint_em": 0.3485148514851485,
 "joint_f1": 0.49722072776980486,
 "joint_prec": 0.5535340849402419,
 "joint_recall": 0.49371462689289236
}{
 "em": 0.6831683168316832,
 "f1": 0.7800329187688032,
 "prec": 0.7928619631986017,
 "recall": 0.78856999087509,
 "sp_em": 0.5069306930693069,
 "sp_f1": 0.6446734673467336,
 "sp_prec": 0.704389438943894,
 "sp_recall": 0.6308722300801505,
 "joint_em": 0.4168316831683168,
 "joint_f1": 0.5340633437550617,
 "joint_prec": 0.5821240698493931,
 "joint_recall": 0.5347836203754389
}{
 "em": 0.7099009900990099,
 "f1": 0.7968461514597358,
 "prec": 0.8093663951884634,
 "recall": 0.8068092511175945,
 "sp_em": 0.49504950495049505,
 "sp_f1": 0.6289706003567377,
 "sp_prec": 0.7040429042904284,
 "sp_recall": 0.600568128241395,
 "joint_em": 0.4207920792079208,
 "joint_f1": 0.5446737587062166,
 "joint_prec": 0.6090272041806555,
 "joint_recall": 0.5302135508385026
}{
 "em": 0.7079207920792079,
 "f1": 0.7969626530360397,
 "prec": 0.8073425613585877,
 "recall": 0.8092730442401673,
 "sp_em": 0.501980198019802,
 "sp_f1": 0.6544837230975832,
 "sp_prec": 0.7090594059405936,
 "sp_recall": 0.6400730787364447,
 "joint_em": 0.4277227722772277,
 "joint_f1": 0.5574285182142532,
 "joint_prec": 0.5995031449735845,
 "joint_recall": 0.5581407491259814
}{
 "em": 0.7099009900990099,
 "f1": 0.795916833111755,
 "prec": 0.8073806041124648,
 "recall": 0.8059816923416042,
 "sp_em": 0.505940594059406,
 "sp_f1": 0.6571800751503714,
 "sp_prec": 0.7002145214521448,
 "sp_recall": 0.6514497878359259,
 "joint_em": 0.4168316831683168,
 "joint_f1": 0.5508389174921976,
 "joint_prec": 0.5899870692163396,
 "joint_recall": 0.5565194438529835
}{
 "em": 0.691089108910891,
 "f1": 0.7866694754840373,
 "prec": 0.7971691732320939,
 "recall": 0.7993377052674737,
 "sp_em": 0.49603960396039604,
 "sp_f1": 0.6716890864910654,
 "sp_prec": 0.7002557755775572,
 "sp_recall": 0.6761621876473358,
 "joint_em": 0.400990099009901,
 "joint_f1": 0.5592247281995603,
 "joint_prec": 0.5894308905182459,
 "joint_recall": 0.5737919929880326
}{
 "em": 0.7019801980198019,
 "f1": 0.7935972104797839,
 "prec": 0.8014668908534226,
 "recall": 0.8079136788883204,
 "sp_em": 0.504950495049505,
 "sp_f1": 0.6658510026826848,
 "sp_prec": 0.703729372937293,
 "sp_recall": 0.6633734087694476,
 "joint_em": 0.4188118811881188,
 "joint_f1": 0.5549683510224586,
 "joint_prec": 0.5866154961782685,
 "joint_recall": 0.5668178380367135
}{
 "em": 0.7029702970297029,
 "f1": 0.8018045500944165,
 "prec": 0.8121420798247017,
 "recall": 0.8159054160157859,
 "sp_em": 0.5178217821782178,
 "sp_f1": 0.6674986674491614,
 "sp_prec": 0.7071947194719466,
 "sp_recall": 0.6620202734559164,
 "joint_em": 0.4168316831683168,
 "joint_f1": 0.5680128990947726,
 "joint_prec": 0.6070458633862305,
 "joint_recall": 0.5750238584627372
}{
 "em": 0.7188118811881188,
 "f1": 0.812175468023922,
 "prec": 0.8214829059954333,
 "recall": 0.8237583549868602,
 "sp_em": 0.5108910891089109,
 "sp_f1": 0.6606887007382047,
 "sp_prec": 0.716336633663366,
 "sp_recall": 0.644545025931164,
 "joint_em": 0.4198019801980198,
 "joint_f1": 0.566964895394102,
 "joint_prec": 0.6175309041015512,
 "joint_recall": 0.5640619755712526
}{
 "em": 0.7168316831683168,
 "f1": 0.8109463306852884,
 "prec": 0.8204470959285844,
 "recall": 0.8230522218735491,
 "sp_em": 0.5079207920792079,
 "sp_f1": 0.6656336952376546,
 "sp_prec": 0.7100495049504949,
 "sp_recall": 0.6577298444130123,
 "joint_em": 0.4207920792079208,
 "joint_f1": 0.5713252112786523,
 "joint_prec": 0.6136561069648219,
 "joint_recall": 0.5756373082122752
}{
  "name": "train_k1",
  "prediction_path": "output/submissions/train_k1",
  "checkpoint_path": "output/checkpoints/train_k1",
  "data_dir": "./data/output",
  "fp16": true,
  "ckpt_id": 0,
  "bert_model": "/data/wuyunzhao/pre-model/chinese_roberta_wwm_large_ext_pytorch",
  "epochs": 10,
  "qat_epochs": 0,
  "batch_size": 2,
  "max_bert_size": 8,
  "eval_batch_size": 2,
  "lr": 1e-05,
  "decay": 1.0,
  "early_stop_epoch": 0,
  "verbose_step": 50,
  "gradient_accumulation_steps": 1,
  "seed": 5,
  "q_update": false,
  "prediction_trans": false,
  "trans_drop": 0.5,
  "trans_heads": 3,
  "input_dim": 768,
  "model_gpu": "2,3",
  "trained_weight": null,
  "type_lambda": 1,
  "sp_lambda": 5,
  "sp_threshold": 0.5,
  "label_type_num": 4,
  "max_query_len": 64,
  "max_doc_len": 512
}{
 "em": 0.6772277227722773,
 "f1": 0.7574507157156164,
 "prec": 0.7627926094748043,
 "recall": 0.7687165260399031,
 "sp_em": 0.41287128712871285,
 "sp_f1": 0.6406075607560745,
 "sp_prec": 0.6797854785478549,
 "sp_recall": 0.6478524280999531,
 "joint_em": 0.33564356435643566,
 "joint_f1": 0.5092249768829095,
 "joint_prec": 0.5306044374267109,
 "joint_recall": 0.5306232639848933
}{
 "em": 0.7039603960396039,
 "f1": 0.7998571834370918,
 "prec": 0.8034635892150184,
 "recall": 0.8179822688253623,
 "sp_em": 0.45445544554455447,
 "sp_f1": 0.651635520694925,
 "sp_prec": 0.7055445544554455,
 "sp_recall": 0.6397842998585574,
 "joint_em": 0.3702970297029703,
 "joint_f1": 0.5456705652913232,
 "joint_prec": 0.5874063433421319,
 "joint_recall": 0.5520874953313679
}{
 "em": 0.7277227722772277,
 "f1": 0.8156284052931942,
 "prec": 0.8220476429659801,
 "recall": 0.8285653634598885,
 "sp_em": 0.4891089108910891,
 "sp_f1": 0.6600028464384888,
 "sp_prec": 0.713224893917963,
 "sp_recall": 0.6485136727958511,
 "joint_em": 0.4188118811881188,
 "joint_f1": 0.5612516760095967,
 "joint_prec": 0.603770704011024,
 "joint_recall": 0.5632375845174135
}{
 "em": 0.7128712871287128,
 "f1": 0.805415545499945,
 "prec": 0.8088482040101985,
 "recall": 0.8225179292256788,
 "sp_em": 0.4752475247524752,
 "sp_f1": 0.6662958823354849,
 "sp_prec": 0.6818324689611817,
 "sp_recall": 0.6847017916077321,
 "joint_em": 0.39900990099009903,
 "joint_f1": 0.5618732086061459,
 "joint_prec": 0.5754476768740118,
 "joint_recall": 0.5905154093961659
}{
 "em": 0.7198019801980198,
 "f1": 0.8095398651827721,
 "prec": 0.815203506487247,
 "recall": 0.8217420435789227,
 "sp_em": 0.48316831683168315,
 "sp_f1": 0.6711383281185249,
 "sp_prec": 0.6968540782649693,
 "sp_recall": 0.6803371051390853,
 "joint_em": 0.41089108910891087,
 "joint_f1": 0.5673583098708825,
 "joint_prec": 0.5912552794160716,
 "joint_recall": 0.5842233531133849
}{
 "em": 0.7158415841584158,
 "f1": 0.8078498521496985,
 "prec": 0.8136377411849325,
 "recall": 0.8189056067845093,
 "sp_em": 0.49306930693069306,
 "sp_f1": 0.6646888260254582,
 "sp_prec": 0.7044141914191416,
 "sp_recall": 0.6616065535124942,
 "joint_em": 0.403960396039604,
 "joint_f1": 0.5623217632293422,
 "joint_prec": 0.5974160053391759,
 "joint_recall": 0.5697030045888479
}{
 "em": 0.7158415841584158,
 "f1": 0.8089154392780867,
 "prec": 0.8178191412531955,
 "recall": 0.8193713838698052,
 "sp_em": 0.4871287128712871,
 "sp_f1": 0.6629602960296018,
 "sp_prec": 0.7107838283828379,
 "sp_recall": 0.6548338048090525,
 "joint_em": 0.4,
 "joint_f1": 0.5617428943186146,
 "joint_prec": 0.6046441816179317,
 "joint_recall": 0.5631435636761537
}{
 "em": 0.7089108910891089,
 "f1": 0.7994019583379579,
 "prec": 0.8058365204034067,
 "recall": 0.810762442068151,
 "sp_em": 0.48118811881188117,
 "sp_f1": 0.6717936793679352,
 "sp_prec": 0.706700848656294,
 "sp_recall": 0.6733651579443657,
 "joint_em": 0.401980198019802,
 "joint_f1": 0.5661570401073016,
 "joint_prec": 0.5959694971840546,
 "joint_recall": 0.5763731406320354
}{
 "em": 0.7118811881188118,
 "f1": 0.8021363288782127,
 "prec": 0.8084476812192288,
 "recall": 0.8134212691753514,
 "sp_em": 0.48217821782178216,
 "sp_f1": 0.6681501007243567,
 "sp_prec": 0.7037211221122108,
 "sp_recall": 0.6690417256011316,
 "joint_em": 0.402970297029703,
 "joint_f1": 0.5652287605117484,
 "joint_prec": 0.5940352983295478,
 "joint_recall": 0.5763159461370756
}{
 "em": 0.7168316831683168,
 "f1": 0.8087036225924185,
 "prec": 0.8139550689392671,
 "recall": 0.8191676069670254,
 "sp_em": 0.4871287128712871,
 "sp_f1": 0.6704188990327591,
 "sp_prec": 0.7030445544554452,
 "sp_recall": 0.6729691183404055,
 "joint_em": 0.4089108910891089,
 "joint_f1": 0.568538674462104,
 "joint_prec": 0.5963549088741253,
 "joint_recall": 0.5798324688419777
}
{
  "name": "train_k1",
  "prediction_path": "output/submissions/train_k1",
  "checkpoint_path": "output/checkpoints/train_k1",
  "data_dir": "./data/output",
  "fp16": true,
  "ckpt_id": 0,
  "bert_model": "/data/wuyunzhao/pre-model/chinese_roberta_wwm_large_ext_pytorch",
  "epochs": 10,
  "qat_epochs": 0,
  "batch_size": 2,
  "max_bert_size": 8,
  "eval_batch_size": 2,
  "lr": 1e-05,
  "decay": 1.0,
  "early_stop_epoch": 0,
  "verbose_step": 50,
  "gradient_accumulation_steps": 1,
  "seed": 5,
  "q_update": false,
  "prediction_trans": false,
  "trans_drop": 0.5,
  "trans_heads": 3,
  "input_dim": 768,
  "model_gpu": "2,3",
  "trained_weight": null,
  "type_lambda": 1,
  "sp_lambda": 5,
  "sp_threshold": 0.5,
  "label_type_num": 4,
  "max_query_len": 64,
  "max_doc_len": 512
}{
  "name": "train_k",
  "prediction_path": "output/submissions/train_k",
  "checkpoint_path": "output/checkpoints/train_k",
  "data_dir": "./data/output",
  "fp16": false,
  "ckpt_id": 0,
  "bert_model": "./chinese_roberta_wwm_large_ext_pytorch",
  "epochs": 10,
  "qat_epochs": 0,
  "batch_size": 2,
  "max_bert_size": 8,
  "eval_batch_size": 2,
  "lr": 1e-05,
  "decay": 1.0,
  "early_stop_epoch": 0,
  "verbose_step": 50,
  "gradient_accumulation_steps": 1,
  "seed": 0,
  "q_update": false,
  "prediction_trans": false,
  "trans_drop": 0.5,
  "trans_heads": 3,
  "input_dim": 768,
  "model_gpu": "0",
  "trained_weight": "./output/checkpoints/train_v1/pred_seed_5_epoch_8_99999.pth",
  "type_lambda": 1,
  "sp_lambda": 5,
  "sp_threshold": 0.5,
  "label_type_num": 4,
  "max_query_len": 64,
  "max_doc_len": 512
}{
  "name": "train_k",
  "prediction_path": "output/submissions/train_k",
  "checkpoint_path": "output/checkpoints/train_k",
  "data_dir": "./data/output",
  "fp16": false,
  "ckpt_id": 0,
  "bert_model": "./chinese_roberta_wwm_large_ext_pytorch",
  "epochs": 10,
  "qat_epochs": 0,
  "batch_size": 2,
  "max_bert_size": 8,
  "eval_batch_size": 2,
  "lr": 1e-05,
  "decay": 1.0,
  "early_stop_epoch": 0,
  "verbose_step": 50,
  "gradient_accumulation_steps": 1,
  "seed": 0,
  "q_update": false,
  "prediction_trans": false,
  "trans_drop": 0.5,
  "trans_heads": 3,
  "input_dim": 768,
  "model_gpu": "0",
  "trained_weight": "./output/checkpoints/train_v1/pred_seed_5_epoch_8_99999.pth",
  "type_lambda": 1,
  "sp_lambda": 5,
  "sp_threshold": 0.5,
  "label_type_num": 4,
  "max_query_len": 64,
  "max_doc_len": 512
}{
  "name": "train_k",
  "prediction_path": "output/submissions/train_k",
  "checkpoint_path": "output/checkpoints/train_k",
  "data_dir": "./data/output",
  "fp16": false,
  "ckpt_id": 0,
  "bert_model": "./chinese_roberta_wwm_large_ext_pytorch",
  "epochs": 10,
  "qat_epochs": 0,
  "batch_size": 2,
  "max_bert_size": 8,
  "eval_batch_size": 2,
  "lr": 1e-05,
  "decay": 1.0,
  "early_stop_epoch": 0,
  "verbose_step": 50,
  "gradient_accumulation_steps": 1,
  "seed": 0,
  "q_update": false,
  "prediction_trans": false,
  "trans_drop": 0.5,
  "trans_heads": 3,
  "input_dim": 768,
  "model_gpu": "0",
  "trained_weight": "./output/checkpoints/train_v1/pred_seed_5_epoch_8_99999.pth",
  "type_lambda": 1,
  "sp_lambda": 5,
  "sp_threshold": 0.5,
  "label_type_num": 4,
  "max_query_len": 64,
  "max_doc_len": 512
}{
  "name": "train_k",
  "prediction_path": "output/submissions/train_k",
  "checkpoint_path": "output/checkpoints/train_k",
  "data_dir": "./data/output",
  "fp16": false,
  "ckpt_id": 0,
  "bert_model": "./chinese_roberta_wwm_large_ext_pytorch",
  "epochs": 10,
  "qat_epochs": 0,
  "batch_size": 2,
  "max_bert_size": 8,
  "eval_batch_size": 2,
  "lr": 1e-05,
  "decay": 1.0,
  "early_stop_epoch": 0,
  "verbose_step": 50,
  "gradient_accumulation_steps": 1,
  "seed": 0,
  "q_update": false,
  "prediction_trans": false,
  "trans_drop": 0.5,
  "trans_heads": 3,
  "input_dim": 768,
  "model_gpu": "0",
  "trained_weight": "./output/checkpoints/train_v1/pred_seed_5_epoch_8_99999.pth",
  "type_lambda": 1,
  "sp_lambda": 5,
  "sp_threshold": 0.5,
  "label_type_num": 4,
  "max_query_len": 64,
  "max_doc_len": 512
}{
  "name": "train_k",
  "prediction_path": "output/submissions/train_k",
  "checkpoint_path": "output/checkpoints/train_k",
  "data_dir": "./data/output",
  "fp16": false,
  "ckpt_id": 0,
  "bert_model": "./chinese_roberta_wwm_large_ext_pytorch",
  "epochs": 10,
  "qat_epochs": 0,
  "batch_size": 2,
  "max_bert_size": 8,
  "eval_batch_size": 2,
  "lr": 1e-05,
  "decay": 1.0,
  "early_stop_epoch": 0,
  "verbose_step": 50,
  "gradient_accumulation_steps": 1,
  "seed": 0,
  "q_update": false,
  "prediction_trans": false,
  "trans_drop": 0.5,
  "trans_heads": 3,
  "input_dim": 768,
  "model_gpu": "0",
  "trained_weight": "./output/checkpoints/train_v1/pred_seed_5_epoch_8_99999.pth",
  "type_lambda": 1,
  "sp_lambda": 5,
  "sp_threshold": 0.5,
  "label_type_num": 4,
  "max_query_len": 64,
  "max_doc_len": 512
}{
  "name": "train_k",
  "prediction_path": "output/submissions/train_k",
  "checkpoint_path": "output/checkpoints/train_k",
  "data_dir": "./data/output",
  "fp16": false,
  "ckpt_id": 0,
  "bert_model": "./chinese_roberta_wwm_large_ext_pytorch",
  "epochs": 10,
  "qat_epochs": 0,
  "batch_size": 2,
  "max_bert_size": 8,
  "eval_batch_size": 2,
  "lr": 1e-05,
  "decay": 1.0,
  "early_stop_epoch": 0,
  "verbose_step": 50,
  "gradient_accumulation_steps": 1,
  "seed": 0,
  "q_update": false,
  "prediction_trans": false,
  "trans_drop": 0.5,
  "trans_heads": 3,
  "input_dim": 768,
  "model_gpu": "0",
  "trained_weight": "./output/checkpoints/train_v1/pred_seed_5_epoch_8_99999.pth",
  "type_lambda": 1,
  "sp_lambda": 5,
  "sp_threshold": 0.5,
  "label_type_num": 4,
  "max_query_len": 64,
  "max_doc_len": 512
}{
  "name": "train_k",
  "prediction_path": "output/submissions/train_k",
  "checkpoint_path": "output/checkpoints/train_k",
  "data_dir": "./data/output",
  "fp16": false,
  "ckpt_id": 0,
  "bert_model": "./chinese_roberta_wwm_large_ext_pytorch",
  "epochs": 10,
  "qat_epochs": 0,
  "batch_size": 2,
  "max_bert_size": 8,
  "eval_batch_size": 2,
  "lr": 1e-05,
  "decay": 1.0,
  "early_stop_epoch": 0,
  "verbose_step": 50,
  "gradient_accumulation_steps": 1,
  "seed": 0,
  "q_update": false,
  "prediction_trans": false,
  "trans_drop": 0.5,
  "trans_heads": 3,
  "input_dim": 768,
  "model_gpu": "0",
  "trained_weight": "./output/checkpoints/train_v1/pred_seed_5_epoch_8_99999.pth",
  "type_lambda": 1,
  "sp_lambda": 5,
  "sp_threshold": 0.5,
  "label_type_num": 4,
  "max_query_len": 64,
  "max_doc_len": 512
}{
  "name": "train_k",
  "prediction_path": "output/submissions/train_k",
  "checkpoint_path": "output/checkpoints/train_k",
  "data_dir": "./data/output",
  "fp16": false,
  "ckpt_id": 0,
  "bert_model": "./chinese_roberta_wwm_large_ext_pytorch",
  "epochs": 10,
  "qat_epochs": 0,
  "batch_size": 2,
  "max_bert_size": 8,
  "eval_batch_size": 2,
  "lr": 1e-05,
  "decay": 1.0,
  "early_stop_epoch": 0,
  "verbose_step": 50,
  "gradient_accumulation_steps": 1,
  "seed": 0,
  "q_update": false,
  "prediction_trans": false,
  "trans_drop": 0.5,
  "trans_heads": 3,
  "input_dim": 768,
  "model_gpu": "0",
  "trained_weight": "./output/checkpoints/train_v1/pred_seed_5_epoch_8_99999.pth",
  "type_lambda": 1,
  "sp_lambda": 5,
  "sp_threshold": 0.5,
  "label_type_num": 4,
  "max_query_len": 64,
  "max_doc_len": 512
}{
  "name": "train_k",
  "prediction_path": "output/submissions/train_k",
  "checkpoint_path": "output/checkpoints/train_k",
  "data_dir": "./data/output",
  "fp16": false,
  "ckpt_id": 0,
  "bert_model": "./chinese_roberta_wwm_large_ext_pytorch",
  "epochs": 10,
  "qat_epochs": 0,
  "batch_size": 2,
  "max_bert_size": 8,
  "eval_batch_size": 2,
  "lr": 1e-05,
  "decay": 1.0,
  "early_stop_epoch": 0,
  "verbose_step": 50,
  "gradient_accumulation_steps": 1,
  "seed": 0,
  "q_update": false,
  "prediction_trans": false,
  "trans_drop": 0.5,
  "trans_heads": 3,
  "input_dim": 768,
  "model_gpu": "0",
  "trained_weight": "./output/checkpoints/train_v1/pred_seed_5_epoch_8_99999.pth",
  "type_lambda": 1,
  "sp_lambda": 5,
  "sp_threshold": 0.5,
  "label_type_num": 4,
  "max_query_len": 64,
  "max_doc_len": 512
}{
  "name": "train_k",
  "prediction_path": "output/submissions/train_k",
  "checkpoint_path": "output/checkpoints/train_k",
  "data_dir": "./data/output",
  "fp16": false,
  "ckpt_id": 0,
  "bert_model": "./chinese_roberta_wwm_large_ext_pytorch",
  "epochs": 10,
  "qat_epochs": 0,
  "batch_size": 2,
  "max_bert_size": 8,
  "eval_batch_size": 2,
  "lr": 1e-05,
  "decay": 1.0,
  "early_stop_epoch": 0,
  "verbose_step": 50,
  "gradient_accumulation_steps": 1,
  "seed": 0,
  "q_update": false,
  "prediction_trans": false,
  "trans_drop": 0.5,
  "trans_heads": 3,
  "input_dim": 768,
  "model_gpu": "0",
  "trained_weight": "./output/checkpoints/train_v1/pred_seed_5_epoch_8_99999.pth",
  "type_lambda": 1,
  "sp_lambda": 5,
  "sp_threshold": 0.5,
  "label_type_num": 4,
  "max_query_len": 64,
  "max_doc_len": 512
}{
  "name": "train_k",
  "prediction_path": "output/submissions/train_k",
  "checkpoint_path": "output/checkpoints/train_k",
  "data_dir": "./data/output",
  "fp16": false,
  "ckpt_id": 0,
  "bert_model": "./chinese_roberta_wwm_large_ext_pytorch",
  "epochs": 10,
  "qat_epochs": 0,
  "batch_size": 2,
  "max_bert_size": 8,
  "eval_batch_size": 2,
  "lr": 1e-05,
  "decay": 1.0,
  "early_stop_epoch": 0,
  "verbose_step": 50,
  "gradient_accumulation_steps": 1,
  "seed": 0,
  "q_update": false,
  "prediction_trans": false,
  "trans_drop": 0.5,
  "trans_heads": 3,
  "input_dim": 768,
  "model_gpu": "0",
  "trained_weight": "./output/checkpoints/train_v1/pred_seed_5_epoch_8_99999.pth",
  "type_lambda": 1,
  "sp_lambda": 5,
  "sp_threshold": 0.5,
  "label_type_num": 4,
  "max_query_len": 64,
  "max_doc_len": 512
}{
  "name": "train_k",
  "prediction_path": "output/submissions/train_k",
  "checkpoint_path": "output/checkpoints/train_k",
  "data_dir": "./data/output",
  "fp16": false,
  "ckpt_id": 0,
  "bert_model": "./chinese_roberta_wwm_large_ext_pytorch",
  "epochs": 10,
  "qat_epochs": 0,
  "batch_size": 2,
  "max_bert_size": 8,
  "eval_batch_size": 2,
  "lr": 1e-05,
  "decay": 1.0,
  "early_stop_epoch": 0,
  "verbose_step": 50,
  "gradient_accumulation_steps": 1,
  "seed": 0,
  "q_update": false,
  "prediction_trans": false,
  "trans_drop": 0.5,
  "trans_heads": 3,
  "input_dim": 768,
  "model_gpu": "0",
  "trained_weight": "./output/checkpoints/train_v1/pred_seed_5_epoch_8_99999.pth",
  "type_lambda": 1,
  "sp_lambda": 5,
  "sp_threshold": 0.5,
  "label_type_num": 4,
  "max_query_len": 64,
  "max_doc_len": 512
}{
  "name": "train_k",
  "prediction_path": "output/submissions/train_k",
  "checkpoint_path": "output/checkpoints/train_k",
  "data_dir": "./data/output",
  "fp16": false,
  "ckpt_id": 0,
  "bert_model": "./chinese_roberta_wwm_large_ext_pytorch",
  "epochs": 10,
  "qat_epochs": 0,
  "batch_size": 2,
  "max_bert_size": 8,
  "eval_batch_size": 2,
  "lr": 1e-05,
  "decay": 1.0,
  "early_stop_epoch": 0,
  "verbose_step": 50,
  "gradient_accumulation_steps": 1,
  "seed": 0,
  "q_update": false,
  "prediction_trans": false,
  "trans_drop": 0.5,
  "trans_heads": 3,
  "input_dim": 768,
  "model_gpu": "0",
  "trained_weight": "./output/checkpoints/train_v1/pred_seed_5_epoch_8_99999.pth",
  "type_lambda": 1,
  "sp_lambda": 5,
  "sp_threshold": 0.5,
  "label_type_num": 4,
  "max_query_len": 64,
  "max_doc_len": 512
}{
  "name": "train_k",
  "prediction_path": "output/submissions/train_k",
  "checkpoint_path": "output/checkpoints/train_k",
  "data_dir": "./data/output",
  "fp16": false,
  "ckpt_id": 0,
  "bert_model": "./chinese_roberta_wwm_large_ext_pytorch",
  "epochs": 10,
  "qat_epochs": 0,
  "batch_size": 2,
  "max_bert_size": 8,
  "eval_batch_size": 2,
  "lr": 1e-05,
  "decay": 1.0,
  "early_stop_epoch": 0,
  "verbose_step": 50,
  "gradient_accumulation_steps": 1,
  "seed": 0,
  "q_update": false,
  "prediction_trans": false,
  "trans_drop": 0.5,
  "trans_heads": 3,
  "input_dim": 768,
  "model_gpu": "0",
  "trained_weight": "./output/checkpoints/train_v1/pred_seed_5_epoch_8_99999.pth",
  "type_lambda": 1,
  "sp_lambda": 5,
  "sp_threshold": 0.5,
  "label_type_num": 4,
  "max_query_len": 64,
  "max_doc_len": 512
}{
  "name": "train_k",
  "prediction_path": "output/submissions/train_k",
  "checkpoint_path": "output/checkpoints/train_k",
  "data_dir": "./data/output",
  "fp16": false,
  "ckpt_id": 0,
  "bert_model": "./chinese_roberta_wwm_large_ext_pytorch",
  "epochs": 10,
  "qat_epochs": 0,
  "batch_size": 2,
  "max_bert_size": 8,
  "eval_batch_size": 2,
  "lr": 1e-05,
  "decay": 1.0,
  "early_stop_epoch": 0,
  "verbose_step": 50,
  "gradient_accumulation_steps": 1,
  "seed": 0,
  "q_update": false,
  "prediction_trans": false,
  "trans_drop": 0.5,
  "trans_heads": 3,
  "input_dim": 768,
  "model_gpu": "0",
  "trained_weight": "./output/checkpoints/train_v1/pred_seed_5_epoch_8_99999.pth",
  "type_lambda": 1,
  "sp_lambda": 5,
  "sp_threshold": 0.5,
  "label_type_num": 4,
  "max_query_len": 64,
  "max_doc_len": 512
}{
  "name": "train_k",
  "prediction_path": "output/submissions/train_k",
  "checkpoint_path": "output/checkpoints/train_k",
  "data_dir": "./data/output",
  "fp16": false,
  "ckpt_id": 0,
  "bert_model": "./chinese_roberta_wwm_large_ext_pytorch",
  "epochs": 10,
  "qat_epochs": 0,
  "batch_size": 2,
  "max_bert_size": 8,
  "eval_batch_size": 2,
  "lr": 1e-05,
  "decay": 1.0,
  "early_stop_epoch": 0,
  "verbose_step": 50,
  "gradient_accumulation_steps": 1,
  "seed": 0,
  "q_update": false,
  "prediction_trans": false,
  "trans_drop": 0.5,
  "trans_heads": 3,
  "input_dim": 768,
  "model_gpu": "0",
  "trained_weight": "./output/checkpoints/train_v1/pred_seed_5_epoch_8_99999.pth",
  "type_lambda": 1,
  "sp_lambda": 5,
  "sp_threshold": 0.5,
  "label_type_num": 4,
  "max_query_len": 64,
  "max_doc_len": 512
}{
  "name": "train_k",
  "prediction_path": "output/submissions/train_k",
  "checkpoint_path": "output/checkpoints/train_k",
  "data_dir": "./data/output",
  "fp16": false,
  "ckpt_id": 0,
  "bert_model": "./chinese_roberta_wwm_large_ext_pytorch",
  "epochs": 10,
  "qat_epochs": 0,
  "batch_size": 2,
  "max_bert_size": 8,
  "eval_batch_size": 2,
  "lr": 1e-05,
  "decay": 1.0,
  "early_stop_epoch": 0,
  "verbose_step": 50,
  "gradient_accumulation_steps": 1,
  "seed": 0,
  "q_update": false,
  "prediction_trans": false,
  "trans_drop": 0.5,
  "trans_heads": 3,
  "input_dim": 768,
  "model_gpu": "0",
  "trained_weight": "./output/checkpoints/train_v1/pred_seed_5_epoch_8_99999.pth",
  "type_lambda": 1,
  "sp_lambda": 5,
  "sp_threshold": 0.5,
  "label_type_num": 4,
  "max_query_len": 64,
  "max_doc_len": 512
}{
  "name": "train_k",
  "prediction_path": "output/submissions/train_k",
  "checkpoint_path": "output/checkpoints/train_k",
  "data_dir": "./data/output",
  "fp16": false,
  "ckpt_id": 0,
  "bert_model": "./chinese_roberta_wwm_large_ext_pytorch",
  "epochs": 10,
  "qat_epochs": 0,
  "batch_size": 2,
  "max_bert_size": 8,
  "eval_batch_size": 12,
  "lr": 1e-05,
  "decay": 1.0,
  "early_stop_epoch": 0,
  "verbose_step": 50,
  "gradient_accumulation_steps": 1,
  "seed": 0,
  "q_update": false,
  "prediction_trans": false,
  "trans_drop": 0.5,
  "trans_heads": 3,
  "input_dim": 768,
  "model_gpu": "0",
  "trained_weight": "./output/checkpoints/train_v1/pred_seed_5_epoch_8_99999.pth",
  "type_lambda": 1,
  "sp_lambda": 5,
  "sp_threshold": 0.5,
  "label_type_num": 4,
  "max_query_len": 64,
  "max_doc_len": 512
}{
  "name": "train_k",
  "prediction_path": "output/submissions/train_k",
  "checkpoint_path": "output/checkpoints/train_k",
  "data_dir": "./data/output",
  "fp16": false,
  "ckpt_id": 0,
  "bert_model": "./chinese_roberta_wwm_large_ext_pytorch",
  "epochs": 10,
  "qat_epochs": 0,
  "batch_size": 2,
  "max_bert_size": 8,
  "eval_batch_size": 12,
  "lr": 1e-05,
  "decay": 1.0,
  "early_stop_epoch": 0,
  "verbose_step": 50,
  "gradient_accumulation_steps": 1,
  "seed": 0,
  "q_update": false,
  "prediction_trans": false,
  "trans_drop": 0.5,
  "trans_heads": 3,
  "input_dim": 768,
  "model_gpu": "0",
  "trained_weight": "./output/checkpoints/train_v1/pred_seed_5_epoch_8_99999.pth",
  "type_lambda": 1,
  "sp_lambda": 5,
  "sp_threshold": 0.5,
  "label_type_num": 4,
  "max_query_len": 64,
  "max_doc_len": 512
}{
  "name": "train_k",
  "prediction_path": "output/submissions/train_k",
  "checkpoint_path": "output/checkpoints/train_k",
  "data_dir": "./data/output",
  "fp16": false,
  "ckpt_id": 0,
  "bert_model": "./chinese_roberta_wwm_large_ext_pytorch",
  "epochs": 10,
  "qat_epochs": 0,
  "batch_size": 2,
  "max_bert_size": 8,
  "eval_batch_size": 12,
  "lr": 1e-05,
  "decay": 1.0,
  "early_stop_epoch": 0,
  "verbose_step": 50,
  "gradient_accumulation_steps": 1,
  "seed": 0,
  "q_update": false,
  "prediction_trans": false,
  "trans_drop": 0.5,
  "trans_heads": 3,
  "input_dim": 768,
  "model_gpu": "0",
  "trained_weight": "./output/checkpoints/train_v1/pred_seed_5_epoch_8_99999.pth",
  "type_lambda": 1,
  "sp_lambda": 5,
  "sp_threshold": 0.5,
  "label_type_num": 4,
  "max_query_len": 64,
  "max_doc_len": 512
}{
  "name": "train_k",
  "prediction_path": "output/submissions/train_k",
  "checkpoint_path": "output/checkpoints/train_k",
  "data_dir": "./data/output",
  "fp16": false,
  "ckpt_id": 0,
  "bert_model": "./chinese_roberta_wwm_large_ext_pytorch",
  "epochs": 10,
  "qat_epochs": 0,
  "batch_size": 2,
  "max_bert_size": 8,
  "eval_batch_size": 12,
  "lr": 1e-05,
  "decay": 1.0,
  "early_stop_epoch": 0,
  "verbose_step": 50,
  "gradient_accumulation_steps": 1,
  "seed": 0,
  "q_update": false,
  "prediction_trans": false,
  "trans_drop": 0.5,
  "trans_heads": 3,
  "input_dim": 768,
  "model_gpu": "0",
  "trained_weight": "./output/checkpoints/train_v1/pred_seed_5_epoch_8_99999.pth",
  "type_lambda": 1,
  "sp_lambda": 5,
  "sp_threshold": 0.5,
  "label_type_num": 4,
  "max_query_len": 64,
  "max_doc_len": 512
}{
  "name": "train_k",
  "prediction_path": "output/submissions/train_k",
  "checkpoint_path": "output/checkpoints/train_k",
  "data_dir": "./data/output",
  "fp16": false,
  "ckpt_id": 0,
  "bert_model": "./chinese_roberta_wwm_large_ext_pytorch",
  "epochs": 10,
  "qat_epochs": 0,
  "batch_size": 2,
  "max_bert_size": 8,
  "eval_batch_size": 12,
  "lr": 1e-05,
  "decay": 1.0,
  "early_stop_epoch": 0,
  "verbose_step": 50,
  "gradient_accumulation_steps": 1,
  "seed": 0,
  "q_update": false,
  "prediction_trans": false,
  "trans_drop": 0.5,
  "trans_heads": 3,
  "input_dim": 768,
  "model_gpu": "0",
  "trained_weight": "./output/checkpoints/train_v1/pred_seed_5_epoch_8_99999.pth",
  "type_lambda": 1,
  "sp_lambda": 5,
  "sp_threshold": 0.5,
  "label_type_num": 4,
  "max_query_len": 64,
  "max_doc_len": 512
}{
  "name": "train_k",
  "prediction_path": "output/submissions/train_k",
  "checkpoint_path": "output/checkpoints/train_k",
  "data_dir": "./data/output",
  "fp16": false,
  "ckpt_id": 0,
  "bert_model": "./chinese_roberta_wwm_large_ext_pytorch",
  "epochs": 10,
  "qat_epochs": 0,
  "batch_size": 2,
  "max_bert_size": 8,
  "eval_batch_size": 12,
  "lr": 1e-05,
  "decay": 1.0,
  "early_stop_epoch": 0,
  "verbose_step": 50,
  "gradient_accumulation_steps": 1,
  "seed": 0,
  "q_update": false,
  "prediction_trans": false,
  "trans_drop": 0.5,
  "trans_heads": 3,
  "input_dim": 768,
  "model_gpu": "0",
  "trained_weight": "./output/checkpoints/train_v1/pred_seed_5_epoch_8_99999.pth",
  "type_lambda": 1,
  "sp_lambda": 5,
  "sp_threshold": 0.5,
  "label_type_num": 4,
  "max_query_len": 64,
  "max_doc_len": 512
}{
  "name": "train_k",
  "prediction_path": "output/submissions/train_k",
  "checkpoint_path": "output/checkpoints/train_k",
  "data_dir": "./data/output",
  "fp16": false,
  "ckpt_id": 0,
  "bert_model": "./chinese_roberta_wwm_large_ext_pytorch",
  "epochs": 10,
  "qat_epochs": 0,
  "batch_size": 2,
  "max_bert_size": 8,
  "eval_batch_size": 12,
  "lr": 1e-05,
  "decay": 1.0,
  "early_stop_epoch": 0,
  "verbose_step": 50,
  "gradient_accumulation_steps": 1,
  "seed": 0,
  "q_update": false,
  "prediction_trans": false,
  "trans_drop": 0.5,
  "trans_heads": 3,
  "input_dim": 768,
  "model_gpu": "0",
  "trained_weight": "./output/checkpoints/train_v1/pred_seed_5_epoch_8_99999.pth",
  "type_lambda": 1,
  "sp_lambda": 5,
  "sp_threshold": 0.5,
  "label_type_num": 4,
  "max_query_len": 64,
  "max_doc_len": 512
}{
  "name": "train_k",
  "prediction_path": "output/submissions/train_k",
  "checkpoint_path": "output/checkpoints/train_k",
  "data_dir": "./data/output",
  "fp16": false,
  "ckpt_id": 0,
  "bert_model": "./chinese_roberta_wwm_large_ext_pytorch",
  "epochs": 10,
  "qat_epochs": 0,
  "batch_size": 2,
  "max_bert_size": 8,
  "eval_batch_size": 12,
  "lr": 1e-05,
  "decay": 1.0,
  "early_stop_epoch": 0,
  "verbose_step": 50,
  "gradient_accumulation_steps": 1,
  "seed": 0,
  "q_update": false,
  "prediction_trans": false,
  "trans_drop": 0.5,
  "trans_heads": 3,
  "input_dim": 768,
  "model_gpu": "0",
  "trained_weight": "./output/checkpoints/train_v1/pred_seed_5_epoch_8_99999.pth",
  "type_lambda": 1,
  "sp_lambda": 5,
  "sp_threshold": 0.5,
  "label_type_num": 4,
  "max_query_len": 64,
  "max_doc_len": 512
}{
  "name": "train_k",
  "prediction_path": "output/submissions/train_k",
  "checkpoint_path": "output/checkpoints/train_k",
  "data_dir": "./data/output",
  "fp16": false,
  "ckpt_id": 0,
  "bert_model": "./chinese_roberta_wwm_large_ext_pytorch",
  "epochs": 10,
  "qat_epochs": 0,
  "batch_size": 2,
  "max_bert_size": 8,
  "eval_batch_size": 12,
  "lr": 1e-05,
  "decay": 1.0,
  "early_stop_epoch": 0,
  "verbose_step": 50,
  "gradient_accumulation_steps": 1,
  "seed": 0,
  "q_update": false,
  "prediction_trans": false,
  "trans_drop": 0.5,
  "trans_heads": 3,
  "input_dim": 768,
  "model_gpu": "0",
  "trained_weight": "./output/checkpoints/train_v1/pred_seed_5_epoch_8_99999.pth",
  "type_lambda": 1,
  "sp_lambda": 5,
  "sp_threshold": 0.5,
  "label_type_num": 4,
  "max_query_len": 64,
  "max_doc_len": 512
}{
  "name": "train_k",
  "prediction_path": "output/submissions/train_k",
  "checkpoint_path": "output/checkpoints/train_k",
  "data_dir": "./data/output",
  "fp16": false,
  "ckpt_id": 0,
  "bert_model": "./chinese_roberta_wwm_large_ext_pytorch",
  "epochs": 10,
  "qat_epochs": 0,
  "batch_size": 2,
  "max_bert_size": 8,
  "eval_batch_size": 12,
  "lr": 1e-05,
  "decay": 1.0,
  "early_stop_epoch": 0,
  "verbose_step": 50,
  "gradient_accumulation_steps": 1,
  "seed": 0,
  "q_update": false,
  "prediction_trans": false,
  "trans_drop": 0.5,
  "trans_heads": 3,
  "input_dim": 768,
  "model_gpu": "0",
  "trained_weight": "./output/checkpoints/train_v1/pred_seed_5_epoch_8_99999.pth",
  "type_lambda": 1,
  "sp_lambda": 5,
  "sp_threshold": 0.5,
  "label_type_num": 4,
  "max_query_len": 64,
  "max_doc_len": 512
}{
  "name": "train_k",
  "prediction_path": "output/submissions/train_k",
  "checkpoint_path": "output/checkpoints/train_k",
  "data_dir": "./data/output",
  "fp16": false,
  "ckpt_id": 0,
  "bert_model": "./chinese_roberta_wwm_large_ext_pytorch",
  "epochs": 10,
  "qat_epochs": 0,
  "batch_size": 2,
  "max_bert_size": 8,
  "eval_batch_size": 12,
  "lr": 1e-05,
  "decay": 1.0,
  "early_stop_epoch": 0,
  "verbose_step": 50,
  "gradient_accumulation_steps": 1,
  "seed": 0,
  "q_update": false,
  "prediction_trans": false,
  "trans_drop": 0.5,
  "trans_heads": 3,
  "input_dim": 768,
  "model_gpu": "0",
  "trained_weight": "./output/checkpoints/train_v1/pred_seed_5_epoch_8_99999.pth",
  "type_lambda": 1,
  "sp_lambda": 5,
  "sp_threshold": 0.5,
  "label_type_num": 4,
  "max_query_len": 64,
  "max_doc_len": 512
}{
  "em":0.9821782178,
  "f1":0.9894698784,
  "prec":0.9904924332,
  "recall":0.9902363986,
  "sp_em":0.8138613861,
  "sp_f1":0.8213570643,
  "sp_prec":0.8392821782,
  "sp_recall":0.8127062706,
  "joint_em":0.8069306931,
  "joint_f1":0.8167506509,
  "joint_prec":0.8344280768,
  "joint_recall":0.8086679918
}