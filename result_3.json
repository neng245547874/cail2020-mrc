{
  "name": "train_k",
  "prediction_path": "output/submissions/train_k",
  "checkpoint_path": "output/checkpoints/train_k",
  "data_dir": "./data/output",
  "fp16": false,
  "ckpt_id": 0,
  "bert_model": "./chinese_roberta_wwm_large_ext_pytorch",
  "epochs": 10,
  "qat_epochs": 0,
  "batch_size": 2,
  "max_bert_size": 8,
  "eval_batch_size": 2,
  "lr": 1e-05,
  "decay": 1.0,
  "early_stop_epoch": 0,
  "verbose_step": 50,
  "gradient_accumulation_steps": 1,
  "seed": 0,
  "q_update": false,
  "prediction_trans": false,
  "trans_drop": 0.5,
  "trans_heads": 3,
  "input_dim": 768,
  "model_gpu": "0",
  "trained_weight": "./output/checkpoints/train_v1/pred_seed_5_epoch_8_99999.pth",
  "type_lambda": 1,
  "sp_lambda": 5,
  "sp_threshold": 0.5,
  "label_type_num": 4,
  "max_query_len": 64,
  "max_doc_len": 512
}{
  "name": "train_k",
  "prediction_path": "output/submissions/train_k",
  "checkpoint_path": "output/checkpoints/train_k",
  "data_dir": "./data/output",
  "fp16": false,
  "ckpt_id": 0,
  "bert_model": "./chinese_roberta_wwm_large_ext_pytorch",
  "epochs": 10,
  "qat_epochs": 0,
  "batch_size": 2,
  "max_bert_size": 8,
  "eval_batch_size": 2,
  "lr": 1e-05,
  "decay": 1.0,
  "early_stop_epoch": 0,
  "verbose_step": 50,
  "gradient_accumulation_steps": 1,
  "seed": 0,
  "q_update": false,
  "prediction_trans": false,
  "trans_drop": 0.5,
  "trans_heads": 3,
  "input_dim": 768,
  "model_gpu": "0",
  "trained_weight": "./output/checkpoints/train_v1/pred_seed_5_epoch_8_99999.pth",
  "type_lambda": 1,
  "sp_lambda": 5,
  "sp_threshold": 0.5,
  "label_type_num": 4,
  "max_query_len": 64,
  "max_doc_len": 512
}{
  "name": "train_k",
  "prediction_path": "output/submissions/train_k",
  "checkpoint_path": "output/checkpoints/train_k",
  "data_dir": "./data/output",
  "fp16": false,
  "ckpt_id": 0,
  "bert_model": "./chinese_roberta_wwm_large_ext_pytorch",
  "epochs": 10,
  "qat_epochs": 0,
  "batch_size": 2,
  "max_bert_size": 8,
  "eval_batch_size": 2,
  "lr": 1e-05,
  "decay": 1.0,
  "early_stop_epoch": 0,
  "verbose_step": 50,
  "gradient_accumulation_steps": 1,
  "seed": 0,
  "q_update": false,
  "prediction_trans": false,
  "trans_drop": 0.5,
  "trans_heads": 3,
  "input_dim": 768,
  "model_gpu": "0",
  "trained_weight": "./output/checkpoints/train_v1/pred_seed_5_epoch_8_99999.pth",
  "type_lambda": 1,
  "sp_lambda": 5,
  "sp_threshold": 0.5,
  "label_type_num": 4,
  "max_query_len": 64,
  "max_doc_len": 512
}{
 "em": 0.7059405940594059,
 "f1": 0.8075440593541473,
 "prec": 0.8135033274340392,
 "recall": 0.8253215786668142,
 "sp_em": 0.45445544554455447,
 "sp_f1": 0.672569839401521,
 "sp_prec": 0.6937647336162179,
 "sp_recall": 0.6867138142385663,
 "joint_em": 0.3603960396039604,
 "joint_f1": 0.5648809016599415,
 "joint_prec": 0.5865813109224303,
 "joint_recall": 0.5920875686226519
}{
  "name": "train_k",
  "prediction_path": "output/submissions/train_k",
  "checkpoint_path": "output/checkpoints/train_k",
  "data_dir": "./data/output",
  "fp16": false,
  "ckpt_id": 0,
  "bert_model": "./chinese_roberta_wwm_large_ext_pytorch",
  "epochs": 10,
  "qat_epochs": 0,
  "batch_size": 2,
  "max_bert_size": 8,
  "eval_batch_size": 2,
  "lr": 1e-05,
  "decay": 1.0,
  "early_stop_epoch": 0,
  "verbose_step": 50,
  "gradient_accumulation_steps": 1,
  "seed": 0,
  "q_update": false,
  "prediction_trans": false,
  "trans_drop": 0.5,
  "trans_heads": 3,
  "input_dim": 768,
  "model_gpu": "0",
  "trained_weight": "./output/checkpoints/train_v1/pred_seed_5_epoch_8_99999.pth",
  "type_lambda": 1,
  "sp_lambda": 5,
  "sp_threshold": 0.5,
  "label_type_num": 4,
  "max_query_len": 64,
  "max_doc_len": 512
}{
  "name": "train_k",
  "prediction_path": "output/submissions/train_k",
  "checkpoint_path": "output/checkpoints/train_k",
  "data_dir": "./data/output",
  "fp16": false,
  "ckpt_id": 0,
  "bert_model": "./chinese_roberta_wwm_large_ext_pytorch",
  "epochs": 10,
  "qat_epochs": 0,
  "batch_size": 2,
  "max_bert_size": 8,
  "eval_batch_size": 2,
  "lr": 1e-05,
  "decay": 1.0,
  "early_stop_epoch": 0,
  "verbose_step": 50,
  "gradient_accumulation_steps": 1,
  "seed": 0,
  "q_update": false,
  "prediction_trans": false,
  "trans_drop": 0.5,
  "trans_heads": 3,
  "input_dim": 768,
  "model_gpu": "0",
  "trained_weight": "./output/checkpoints/train_v1/pred_seed_5_epoch_8_99999.pth",
  "type_lambda": 1,
  "sp_lambda": 5,
  "sp_threshold": 0.5,
  "label_type_num": 4,
  "max_query_len": 64,
  "max_doc_len": 512
}{
  "name": "train_k",
  "prediction_path": "output/submissions/train_k",
  "checkpoint_path": "output/checkpoints/train_k",
  "data_dir": "./data/output",
  "fp16": false,
  "ckpt_id": 0,
  "bert_model": "./chinese_roberta_wwm_large_ext_pytorch",
  "epochs": 10,
  "qat_epochs": 0,
  "batch_size": 2,
  "max_bert_size": 8,
  "eval_batch_size": 2,
  "lr": 1e-05,
  "decay": 1.0,
  "early_stop_epoch": 0,
  "verbose_step": 50,
  "gradient_accumulation_steps": 1,
  "seed": 0,
  "q_update": false,
  "prediction_trans": false,
  "trans_drop": 0.5,
  "trans_heads": 3,
  "input_dim": 768,
  "model_gpu": "0",
  "trained_weight": "./output/checkpoints/train_v1/pred_seed_5_epoch_8_99999.pth",
  "type_lambda": 1,
  "sp_lambda": 5,
  "sp_threshold": 0.5,
  "label_type_num": 4,
  "max_query_len": 64,
  "max_doc_len": 512
}{
 "em": 0.14455445544554454,
 "f1": 0.14455445544554454,
 "prec": 0.14455445544554454,
 "recall": 0.14455445544554454,
 "sp_em": 0.45445544554455447,
 "sp_f1": 0.672569839401521,
 "sp_prec": 0.6937647336162179,
 "sp_recall": 0.6867138142385663,
 "joint_em": 0.04554455445544554,
 "joint_f1": 0.10658565307080156,
 "joint_prec": 0.10737270155586986,
 "joint_recall": 0.11176096181046677
}{
  "name": "train_k",
  "prediction_path": "output/submissions/train_k",
  "checkpoint_path": "output/checkpoints/train_k",
  "data_dir": "./data/output",
  "fp16": false,
  "ckpt_id": 0,
  "bert_model": "./chinese_roberta_wwm_large_ext_pytorch",
  "epochs": 10,
  "qat_epochs": 0,
  "batch_size": 2,
  "max_bert_size": 8,
  "eval_batch_size": 2,
  "lr": 1e-05,
  "decay": 1.0,
  "early_stop_epoch": 0,
  "verbose_step": 50,
  "gradient_accumulation_steps": 1,
  "seed": 0,
  "q_update": false,
  "prediction_trans": false,
  "trans_drop": 0.5,
  "trans_heads": 3,
  "input_dim": 768,
  "model_gpu": "0",
  "trained_weight": "./output/checkpoints/train_v1/pred_seed_5_epoch_8_99999.pth",
  "type_lambda": 1,
  "sp_lambda": 5,
  "sp_threshold": 0.5,
  "label_type_num": 4,
  "max_query_len": 64,
  "max_doc_len": 512
}{
 "em": 0.7059405940594059,
 "f1": 0.8075440593541473,
 "prec": 0.8135033274340392,
 "recall": 0.8253215786668142,
 "sp_em": 0.45445544554455447,
 "sp_f1": 0.672569839401521,
 "sp_prec": 0.6937647336162179,
 "sp_recall": 0.6867138142385663,
 "joint_em": 0.3603960396039604,
 "joint_f1": 0.5648809016599415,
 "joint_prec": 0.5865813109224303,
 "joint_recall": 0.5920875686226519
}{
  "name": "train_k",
  "prediction_path": "output/submissions/train_k",
  "checkpoint_path": "output/checkpoints/train_k",
  "data_dir": "./data/output",
  "fp16": false,
  "ckpt_id": 0,
  "bert_model": "./chinese_roberta_wwm_large_ext_pytorch",
  "epochs": 10,
  "qat_epochs": 0,
  "batch_size": 2,
  "max_bert_size": 8,
  "eval_batch_size": 2,
  "lr": 1e-05,
  "decay": 1.0,
  "early_stop_epoch": 0,
  "verbose_step": 50,
  "gradient_accumulation_steps": 1,
  "seed": 0,
  "q_update": false,
  "prediction_trans": false,
  "trans_drop": 0.5,
  "trans_heads": 3,
  "input_dim": 768,
  "model_gpu": "0",
  "trained_weight": "./output/checkpoints/train_v1/pred_seed_5_epoch_8_99999.pth",
  "type_lambda": 1,
  "sp_lambda": 5,
  "sp_threshold": 0.5,
  "label_type_num": 4,
  "max_query_len": 64,
  "max_doc_len": 512
}{
  "name": "train_k",
  "prediction_path": "output/submissions/train_k",
  "checkpoint_path": "output/checkpoints/train_k",
  "data_dir": "./data/output",
  "fp16": false,
  "ckpt_id": 0,
  "bert_model": "./chinese_roberta_wwm_large_ext_pytorch",
  "epochs": 10,
  "qat_epochs": 0,
  "batch_size": 2,
  "max_bert_size": 8,
  "eval_batch_size": 2,
  "lr": 1e-05,
  "decay": 1.0,
  "early_stop_epoch": 0,
  "verbose_step": 50,
  "gradient_accumulation_steps": 1,
  "seed": 0,
  "q_update": false,
  "prediction_trans": false,
  "trans_drop": 0.5,
  "trans_heads": 3,
  "input_dim": 768,
  "model_gpu": "0",
  "trained_weight": "./output/checkpoints/train_v1/pred_seed_5_epoch_8_99999.pth",
  "type_lambda": 1,
  "sp_lambda": 5,
  "sp_threshold": 0.5,
  "label_type_num": 4,
  "max_query_len": 64,
  "max_doc_len": 512
}{
  "name": "train_k",
  "prediction_path": "output/submissions/train_k",
  "checkpoint_path": "output/checkpoints/train_k",
  "data_dir": "./data/output",
  "fp16": false,
  "ckpt_id": 0,
  "bert_model": "./chinese_roberta_wwm_large_ext_pytorch",
  "epochs": 10,
  "qat_epochs": 0,
  "batch_size": 2,
  "max_bert_size": 8,
  "eval_batch_size": 2,
  "lr": 1e-05,
  "decay": 1.0,
  "early_stop_epoch": 0,
  "verbose_step": 50,
  "gradient_accumulation_steps": 1,
  "seed": 0,
  "q_update": false,
  "prediction_trans": false,
  "trans_drop": 0.5,
  "trans_heads": 3,
  "input_dim": 768,
  "model_gpu": "0",
  "trained_weight": "./output/checkpoints/train_v1/pred_seed_5_epoch_8_99999.pth",
  "type_lambda": 1,
  "sp_lambda": 5,
  "sp_threshold": 0.5,
  "label_type_num": 4,
  "max_query_len": 64,
  "max_doc_len": 512
}{
  "name": "train_k",
  "prediction_path": "output/submissions/train_k",
  "checkpoint_path": "output/checkpoints/train_k",
  "data_dir": "./data/output",
  "fp16": false,
  "ckpt_id": 0,
  "bert_model": "./chinese_roberta_wwm_large_ext_pytorch",
  "epochs": 10,
  "qat_epochs": 0,
  "batch_size": 2,
  "max_bert_size": 8,
  "eval_batch_size": 2,
  "lr": 1e-05,
  "decay": 1.0,
  "early_stop_epoch": 0,
  "verbose_step": 50,
  "gradient_accumulation_steps": 1,
  "seed": 0,
  "q_update": false,
  "prediction_trans": false,
  "trans_drop": 0.5,
  "trans_heads": 3,
  "input_dim": 768,
  "model_gpu": "0",
  "trained_weight": "./output/checkpoints/train_v1/pred_seed_5_epoch_8_99999.pth",
  "type_lambda": 1,
  "sp_lambda": 5,
  "sp_threshold": 0.5,
  "label_type_num": 4,
  "max_query_len": 64,
  "max_doc_len": 512
}{
 "em": 0.7059405940594059,
 "f1": 0.8075440593541473,
 "prec": 0.8135033274340392,
 "recall": 0.8253215786668142,
 "sp_em": 0.45445544554455447,
 "sp_f1": 0.672569839401521,
 "sp_prec": 0.6937647336162179,
 "sp_recall": 0.6867138142385663,
 "joint_em": 0.3603960396039604,
 "joint_f1": 0.5648809016599415,
 "joint_prec": 0.5865813109224303,
 "joint_recall": 0.5920875686226519
}{
  "name": "train_k",
  "prediction_path": "output/submissions/train_k",
  "checkpoint_path": "output/checkpoints/train_k",
  "data_dir": "./data/output",
  "fp16": false,
  "ckpt_id": 0,
  "bert_model": "./chinese_roberta_wwm_large_ext_pytorch",
  "epochs": 10,
  "qat_epochs": 0,
  "batch_size": 2,
  "max_bert_size": 8,
  "eval_batch_size": 2,
  "lr": 1e-05,
  "decay": 1.0,
  "early_stop_epoch": 0,
  "verbose_step": 50,
  "gradient_accumulation_steps": 1,
  "seed": 0,
  "q_update": false,
  "prediction_trans": false,
  "trans_drop": 0.5,
  "trans_heads": 3,
  "input_dim": 768,
  "model_gpu": "0",
  "trained_weight": "./output/checkpoints/train_v1/pred_seed_5_epoch_8_99999.pth",
  "type_lambda": 1,
  "sp_lambda": 5,
  "sp_threshold": 0.5,
  "label_type_num": 4,
  "max_query_len": 64,
  "max_doc_len": 512
}{
  "name": "train_k",
  "prediction_path": "output/submissions/train_k",
  "checkpoint_path": "output/checkpoints/train_k",
  "data_dir": "./data/output",
  "fp16": false,
  "ckpt_id": 0,
  "bert_model": "./chinese_roberta_wwm_large_ext_pytorch",
  "epochs": 10,
  "qat_epochs": 0,
  "batch_size": 2,
  "max_bert_size": 8,
  "eval_batch_size": 2,
  "lr": 1e-05,
  "decay": 1.0,
  "early_stop_epoch": 0,
  "verbose_step": 50,
  "gradient_accumulation_steps": 1,
  "seed": 0,
  "q_update": false,
  "prediction_trans": false,
  "trans_drop": 0.5,
  "trans_heads": 3,
  "input_dim": 768,
  "model_gpu": "0",
  "trained_weight": "./output/checkpoints/train_v1/pred_seed_5_epoch_8_99999.pth",
  "type_lambda": 1,
  "sp_lambda": 5,
  "sp_threshold": 0.5,
  "label_type_num": 4,
  "max_query_len": 64,
  "max_doc_len": 512
}{
 "em": 0.7059405940594059,
 "f1": 0.8075440593541473,
 "prec": 0.8135033274340392,
 "recall": 0.8253215786668142,
 "sp_em": 0.45445544554455447,
 "sp_f1": 0.672569839401521,
 "sp_prec": 0.6937647336162179,
 "sp_recall": 0.6867138142385663,
 "joint_em": 0.3603960396039604,
 "joint_f1": 0.5648809016599415,
 "joint_prec": 0.5865813109224303,
 "joint_recall": 0.5920875686226519
}