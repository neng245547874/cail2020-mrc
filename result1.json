{
  "name": "train_v1",
  "prediction_path": "output/submissions/train_v1",
  "checkpoint_path": "output/checkpoints/train_v1",
  "data_dir": "./data/output",
  "fp16": false,
  "ckpt_id": 0,
  "bert_model": "/data/wuyunzhao/pre-model/chinese_roberta_wwm_large_ext_pytorch",
  "epochs": 5,
  "qat_epochs": 0,
  "batch_size": 2,
  "max_bert_size": 8,
  "eval_batch_size": 2,
  "lr": 1e-05,
  "decay": 1.0,
  "early_stop_epoch": 0,
  "verbose_step": 50,
  "gradient_accumulation_steps": 1,
  "seed": 5,
  "q_update": false,
  "prediction_trans": false,
  "trans_drop": 0.5,
  "trans_heads": 3,
  "input_dim": 768,
  "model_gpu": "0,1",
  "trained_weight": null,
  "type_lambda": 1,
  "sp_lambda": 5,
  "sp_threshold": 0.5,
  "label_type_num": 4,
  "max_query_len": 50,
  "max_doc_len": 512
}{
 "em": 0.6841584158415842,
 "f1": 0.7711381558351149,
 "prec": 0.7764679960495187,
 "recall": 0.7866791994249704,
 "sp_em": 0.43663366336633663,
 "sp_f1": 0.6415737288014505,
 "sp_prec": 0.7005233380480902,
 "sp_recall": 0.6294672324375293,
 "joint_em": 0.3613861386138614,
 "joint_f1": 0.528434572575316,
 "joint_prec": 0.573479148293945,
 "joint_recall": 0.5329695016885219
}{
 "em": 0.7059405940594059,
 "f1": 0.795097501979097,
 "prec": 0.802625773666224,
 "recall": 0.8044930220125222,
 "sp_em": 0.4782178217821782,
 "sp_f1": 0.6516041604160404,
 "sp_prec": 0.7153300330032999,
 "sp_recall": 0.6314309288071666,
 "joint_em": 0.39900990099009903,
 "joint_f1": 0.5483829625085359,
 "joint_prec": 0.6009932904728214,
 "joint_recall": 0.5416964866904791
}{
 "em": 0.7118811881188118,
 "f1": 0.8101051662799399,
 "prec": 0.8150865970638809,
 "recall": 0.824125264535565,
 "sp_em": 0.4900990099009901,
 "sp_f1": 0.6613728515708703,
 "sp_prec": 0.7100660066006598,
 "sp_recall": 0.6521239981140973,
 "joint_em": 0.403960396039604,
 "joint_f1": 0.5628330376234532,
 "joint_prec": 0.6023718525153027,
 "joint_recall": 0.5683616757496209
}{
 "em": 0.7089108910891089,
 "f1": 0.8081644695017728,
 "prec": 0.8138096425886302,
 "recall": 0.8210494925813143,
 "sp_em": 0.4910891089108911,
 "sp_f1": 0.6656261340419743,
 "sp_prec": 0.7200196448216246,
 "sp_recall": 0.6498727015558701,
 "joint_em": 0.39702970297029705,
 "joint_f1": 0.5659988148719831,
 "joint_prec": 0.612272334741418,
 "joint_recall": 0.563989671480247
}{
 "em": 0.7069306930693069,
 "f1": 0.8075554395317066,
 "prec": 0.8129634246043889,
 "recall": 0.8220062155238715,
 "sp_em": 0.4891089108910891,
 "sp_f1": 0.667181948964126,
 "sp_prec": 0.7176567656765672,
 "sp_recall": 0.6545426685525696,
 "joint_em": 0.39801980198019804,
 "joint_f1": 0.564255451892089,
 "joint_prec": 0.6059778989452482,
 "joint_recall": 0.566874188855812
}{
  "name": "train_v1",
  "prediction_path": "output/submissions/train_v1",
  "checkpoint_path": "output/checkpoints/train_v1",
  "data_dir": "./data/output",
  "fp16": false,
  "ckpt_id": 0,
  "bert_model": "/data/wuyunzhao/pre-model/chinese_roberta_wwm_large_ext_pytorch",
  "epochs": 5,
  "qat_epochs": 0,
  "batch_size": 2,
  "max_bert_size": 8,
  "eval_batch_size": 2,
  "lr": 2e-05,
  "decay": 1.0,
  "early_stop_epoch": 0,
  "verbose_step": 50,
  "gradient_accumulation_steps": 1,
  "seed": 5,
  "q_update": false,
  "prediction_trans": false,
  "trans_drop": 0.5,
  "trans_heads": 3,
  "input_dim": 768,
  "model_gpu": "0,1",
  "trained_weight": null,
  "type_lambda": 1,
  "sp_lambda": 5,
  "sp_threshold": 0.5,
  "label_type_num": 4,
  "max_query_len": 50,
  "max_doc_len": 512
}{
 "em": 0.6594059405940594,
 "f1": 0.7476837491637927,
 "prec": 0.7512860709382814,
 "recall": 0.7643442246152305,
 "sp_em": 0.4198019801980198,
 "sp_f1": 0.5736972983012585,
 "sp_prec": 0.6552805280528052,
 "sp_recall": 0.5435997171145684,
 "joint_em": 0.3455445544554455,
 "joint_f1": 0.4671702484917605,
 "joint_prec": 0.5271413947971326,
 "joint_recall": 0.45671648858802916
}{
 "em": 0.6524752475247525,
 "f1": 0.7749972492741871,
 "prec": 0.7742770552226641,
 "recall": 0.798173629601904,
 "sp_em": 0.47029702970297027,
 "sp_f1": 0.6574989806672964,
 "sp_prec": 0.7007920792079201,
 "sp_recall": 0.652293729372937,
 "joint_em": 0.3673267326732673,
 "joint_f1": 0.53542494112681,
 "joint_prec": 0.5636056022124052,
 "joint_recall": 0.554493129241317
}{
 "em": 0.69009900990099,
 "f1": 0.8014177512540419,
 "prec": 0.8062520222733817,
 "recall": 0.8158924427389936,
 "sp_em": 0.4861386138613861,
 "sp_f1": 0.6698648436272185,
 "sp_prec": 0.7077911362564822,
 "sp_recall": 0.6656812824139556,
 "joint_em": 0.38811881188118813,
 "joint_f1": 0.5594949814993196,
 "joint_prec": 0.5926925567844573,
 "joint_recall": 0.5693420080891981
}{
 "em": 0.689108910891089,
 "f1": 0.7944922981941746,
 "prec": 0.7984935989634919,
 "recall": 0.8124073099537039,
 "sp_em": 0.48118811881188117,
 "sp_f1": 0.6569974854628304,
 "sp_prec": 0.6970108439415366,
 "sp_recall": 0.6550471475719,
 "joint_em": 0.39801980198019804,
 "joint_f1": 0.553419299242383,
 "joint_prec": 0.58494497857406,
 "joint_recall": 0.5679599765296869
}{
 "em": 0.694059405940594,
 "f1": 0.8037292648092037,
 "prec": 0.8075362703605199,
 "recall": 0.8206980049971027,
 "sp_em": 0.49603960396039604,
 "sp_f1": 0.6700472904433282,
 "sp_prec": 0.7027510608203669,
 "sp_recall": 0.6697831211692593,
 "joint_em": 0.4,
 "joint_f1": 0.5681513228982826,
 "joint_prec": 0.594192277065316,
 "joint_recall": 0.5823145006449773
}{
  "name": "train_v1",
  "prediction_path": "output/submissions/train_v1",
  "checkpoint_path": "output/checkpoints/train_v1",
  "data_dir": "./data/output",
  "fp16": false,
  "ckpt_id": 0,
  "bert_model": "/data/wuyunzhao/pre-model/chinese_roberta_wwm_large_ext_pytorch",
  "epochs": 8,
  "qat_epochs": 0,
  "batch_size": 2,
  "max_bert_size": 8,
  "eval_batch_size": 2,
  "lr": 5e-06,
  "decay": 1.0,
  "early_stop_epoch": 0,
  "verbose_step": 50,
  "gradient_accumulation_steps": 1,
  "seed": 5,
  "q_update": false,
  "prediction_trans": false,
  "trans_drop": 0.5,
  "trans_heads": 3,
  "input_dim": 768,
  "model_gpu": "0,1",
  "trained_weight": null,
  "type_lambda": 1,
  "sp_lambda": 5,
  "sp_threshold": 0.5,
  "label_type_num": 4,
  "max_query_len": 50,
  "max_doc_len": 512
}{
  "name": "train_v1",
  "prediction_path": "output/submissions/train_v1",
  "checkpoint_path": "output/checkpoints/train_v1",
  "data_dir": "./data/output",
  "fp16": false,
  "ckpt_id": 0,
  "bert_model": "/data/wuyunzhao/pre-model/chinese_roberta_wwm_large_ext_pytorch",
  "epochs": 8,
  "qat_epochs": 0,
  "batch_size": 2,
  "max_bert_size": 8,
  "eval_batch_size": 2,
  "lr": 5e-06,
  "decay": 1.0,
  "early_stop_epoch": 0,
  "verbose_step": 50,
  "gradient_accumulation_steps": 1,
  "seed": 0,
  "q_update": false,
  "prediction_trans": false,
  "trans_drop": 0.5,
  "trans_heads": 3,
  "input_dim": 768,
  "model_gpu": "2,3/",
  "trained_weight": null,
  "type_lambda": 1,
  "sp_lambda": 5,
  "sp_threshold": 0.5,
  "label_type_num": 4,
  "max_query_len": 50,
  "max_doc_len": 512
}{
 "em": 0.6603960396039604,
 "f1": 0.7555982326714389,
 "prec": 0.7594852635781628,
 "recall": 0.769620802709261,
 "sp_em": 0.37623762376237624,
 "sp_f1": 0.5840554769762689,
 "sp_prec": 0.6970132013201317,
 "sp_recall": 0.5422371522866571,
 "joint_em": 0.3089108910891089,
 "joint_f1": 0.47951488895628375,
 "joint_prec": 0.5610486866192975,
 "joint_recall": 0.4605814090772331
}{
 "em": 0.698019801980198,
 "f1": 0.7922633546876462,
 "prec": 0.7967182968726095,
 "recall": 0.8078608236444119,
 "sp_em": 0.47029702970297027,
 "sp_f1": 0.6500195733859094,
 "sp_prec": 0.7143045733144743,
 "sp_recall": 0.6326025459688828,
 "joint_em": 0.38712871287128714,
 "joint_f1": 0.5455095959551731,
 "joint_prec": 0.5907638011737951,
 "joint_recall": 0.5479956606149455
}{
 "em": 0.6792079207920793,
 "f1": 0.7738932626408783,
 "prec": 0.7805965077108229,
 "recall": 0.7854019225942497,
 "sp_em": 0.4772277227722772,
 "sp_f1": 0.6284770619919128,
 "sp_prec": 0.7046204620462044,
 "sp_recall": 0.6023220179160778,
 "joint_em": 0.3910891089108911,
 "joint_f1": 0.5217854595394936,
 "joint_prec": 0.5780721498424567,
 "joint_recall": 0.5139294530682764
}{
 "em": 0.69009900990099,
 "f1": 0.7881703385630368,
 "prec": 0.7944307920483672,
 "recall": 0.8053698303088247,
 "sp_em": 0.47227722772277225,
 "sp_f1": 0.6535835012072624,
 "sp_prec": 0.6995285242809988,
 "sp_recall": 0.6470579915134372,
 "joint_em": 0.38415841584158417,
 "joint_f1": 0.5447131142469606,
 "joint_prec": 0.5817358486365417,
 "joint_recall": 0.5541420123446689
}{
 "em": 0.7019801980198019,
 "f1": 0.7958138413171946,
 "prec": 0.8025052585440857,
 "recall": 0.8087436528180175,
 "sp_em": 0.47227722772277225,
 "sp_f1": 0.6639621874275323,
 "sp_prec": 0.6976897689768968,
 "sp_recall": 0.670077793493635,
 "joint_em": 0.39702970297029705,
 "joint_f1": 0.5551026449424004,
 "joint_prec": 0.5826544756129427,
 "joint_recall": 0.573694636525076
}{
 "em": 0.696039603960396,
 "f1": 0.7947652845337888,
 "prec": 0.8028009865841296,
 "recall": 0.806440125255892,
 "sp_em": 0.4861386138613861,
 "sp_f1": 0.644458198567108,
 "sp_prec": 0.7104702970297027,
 "sp_recall": 0.6232296086751528,
 "joint_em": 0.39702970297029705,
 "joint_f1": 0.54042033480403,
 "joint_prec": 0.5948835536278737,
 "joint_recall": 0.534705572744054
}{
 "em": 0.694059405940594,
 "f1": 0.7928531847343409,
 "prec": 0.8022734238798012,
 "recall": 0.8029948768849009,
 "sp_em": 0.4910891089108911,
 "sp_f1": 0.6598835982499334,
 "sp_prec": 0.7120497406883541,
 "sp_recall": 0.6486586515794436,
 "joint_em": 0.39801980198019804,
 "joint_f1": 0.5512575069076922,
 "joint_prec": 0.5971907526297184,
 "joint_recall": 0.5522745399114839
}{
 "em": 0.693069306930693,
 "f1": 0.792263506507183,
 "prec": 0.8005154799563399,
 "recall": 0.8025567437858766,
 "sp_em": 0.49207920792079207,
 "sp_f1": 0.661462041808575,
 "sp_prec": 0.7102416313059872,
 "sp_recall": 0.6520249882131071,
 "joint_em": 0.39900990099009903,
 "joint_f1": 0.552324893136111,
 "joint_prec": 0.5937447705415386,
 "joint_recall": 0.5547228918895387
}{
  "name": "train_v1",
  "prediction_path": "output/submissions/train_v1",
  "checkpoint_path": "output/checkpoints/train_v1",
  "data_dir": "./data/output",
  "fp16": false,
  "ckpt_id": 0,
  "bert_model": "/data/wuyunzhao/pre-model/chinese_roberta_wwm_large_ext_pytorch",
  "epochs": 10,
  "qat_epochs": 0,
  "batch_size": 2,
  "max_bert_size": 8,
  "eval_batch_size": 2,
  "lr": 1.2e-05,
  "decay": 1.0,
  "early_stop_epoch": 0,
  "verbose_step": 50,
  "gradient_accumulation_steps": 1,
  "seed": 0,
  "q_update": false,
  "prediction_trans": false,
  "trans_drop": 0.5,
  "trans_heads": 3,
  "input_dim": 768,
  "model_gpu": "2,3/",
  "trained_weight": null,
  "type_lambda": 1,
  "sp_lambda": 5,
  "sp_threshold": 0.5,
  "label_type_num": 4,
  "max_query_len": 64,
  "max_doc_len": 512
}{
 "em": 0.6653465346534654,
 "f1": 0.7485230079472956,
 "prec": 0.7514609347501418,
 "recall": 0.7678040420864952,
 "sp_em": 0.43366336633663366,
 "sp_f1": 0.6217056705670561,
 "sp_prec": 0.6901980198019801,
 "sp_recall": 0.6039132484677041,
 "joint_em": 0.35346534653465345,
 "joint_f1": 0.495859616868908,
 "joint_prec": 0.536946415915564,
 "joint_recall": 0.5020373466213468
}{
 "em": 0.7069306930693069,
 "f1": 0.7930541371393416,
 "prec": 0.7999378491571995,
 "recall": 0.8062466565464813,
 "sp_em": 0.4594059405940594,
 "sp_f1": 0.6496018887603034,
 "sp_prec": 0.7148184818481838,
 "sp_recall": 0.6294648750589344,
 "joint_em": 0.3801980198019802,
 "joint_f1": 0.543312020398224,
 "joint_prec": 0.5972065522179818,
 "joint_recall": 0.5388811789812784
}{
 "em": 0.7079207920792079,
 "f1": 0.8064231993193225,
 "prec": 0.8107538052278576,
 "recall": 0.8234170488931677,
 "sp_em": 0.48415841584158414,
 "sp_f1": 0.6452143071449992,
 "sp_prec": 0.7059217350306457,
 "sp_recall": 0.624926921263555,
 "joint_em": 0.401980198019802,
 "joint_f1": 0.5497711940260966,
 "joint_prec": 0.5988485363687684,
 "joint_recall": 0.5475143445030292
}{
 "em": 0.6821782178217822,
 "f1": 0.7909505909924914,
 "prec": 0.7947575081335718,
 "recall": 0.8096210251989492,
 "sp_em": 0.47425742574257423,
 "sp_f1": 0.6392717128855727,
 "sp_prec": 0.7058580858085803,
 "sp_recall": 0.6176025459688824,
 "joint_em": 0.3821782178217822,
 "joint_f1": 0.5331801992240315,
 "joint_prec": 0.5867366342331778,
 "joint_recall": 0.5284538921295244
}{
 "em": 0.7089108910891089,
 "f1": 0.8141239867727512,
 "prec": 0.8218816423031201,
 "recall": 0.8309298295309758,
 "sp_em": 0.48118811881188117,
 "sp_f1": 0.6620347034703459,
 "sp_prec": 0.7157543611504005,
 "sp_recall": 0.646133899104196,
 "joint_em": 0.3940594059405941,
 "joint_f1": 0.5615452419504083,
 "joint_prec": 0.6077539902141276,
 "joint_recall": 0.5633586161303478
}{
 "em": 0.6772277227722773,
 "f1": 0.7907437774279508,
 "prec": 0.7950983795606127,
 "recall": 0.8136941207739772,
 "sp_em": 0.4643564356435644,
 "sp_f1": 0.6703612668959191,
 "sp_prec": 0.6916360207449311,
 "sp_recall": 0.6806883545497403,
 "joint_em": 0.36237623762376237,
 "joint_f1": 0.5576774799115256,
 "joint_prec": 0.5775800153369157,
 "joint_recall": 0.5858935633445661
}{
 "em": 0.7,
 "f1": 0.8048928019199747,
 "prec": 0.8079730711752391,
 "recall": 0.8237012176414915,
 "sp_em": 0.4792079207920792,
 "sp_f1": 0.6528456417070266,
 "sp_prec": 0.7105987741631302,
 "sp_recall": 0.6333946251768034,
 "joint_em": 0.3920792079207921,
 "joint_f1": 0.550162999168571,
 "joint_prec": 0.5963500015585894,
 "joint_recall": 0.547872826921478
}{
 "em": 0.696039603960396,
 "f1": 0.8009304588703594,
 "prec": 0.8072738620881639,
 "recall": 0.8179186117878079,
 "sp_em": 0.47029702970297027,
 "sp_f1": 0.6675890446187462,
 "sp_prec": 0.708303473204463,
 "sp_recall": 0.6628995756718528,
 "joint_em": 0.38415841584158417,
 "joint_f1": 0.5614980984060685,
 "joint_prec": 0.5946468019510645,
 "joint_recall": 0.5724852538756587
}{
 "em": 0.699009900990099,
 "f1": 0.7999426795055372,
 "prec": 0.8038773637605275,
 "recall": 0.8193869468978119,
 "sp_em": 0.47425742574257423,
 "sp_f1": 0.672729525699821,
 "sp_prec": 0.7048408769448372,
 "sp_recall": 0.6743352192362091,
 "joint_em": 0.3821782178217822,
 "joint_f1": 0.5648178602247483,
 "joint_prec": 0.5894379501062131,
 "joint_recall": 0.5846803798787218
}{
 "em": 0.7039603960396039,
 "f1": 0.8040233814301172,
 "prec": 0.8094229992330932,
 "recall": 0.8203178956182569,
 "sp_em": 0.48217821782178216,
 "sp_f1": 0.6684271284271268,
 "sp_prec": 0.7104809052333801,
 "sp_recall": 0.6616949552098064,
 "joint_em": 0.3940594059405941,
 "joint_f1": 0.5640682416449431,
 "joint_prec": 0.5971342268097201,
 "joint_recall": 0.5740589287820234
}{
  "name": "train_v1",
  "prediction_path": "output/submissions/train_v1",
  "checkpoint_path": "output/checkpoints/train_v1",
  "data_dir": "./data/output",
  "fp16": false,
  "ckpt_id": 0,
  "bert_model": "/data/wuyunzhao/pre-model/chinese_roberta_wwm_large_ext_pytorch",
  "epochs": 8,
  "qat_epochs": 0,
  "batch_size": 2,
  "max_bert_size": 8,
  "eval_batch_size": 2,
  "lr": 1e-05,
  "decay": 1.0,
  "early_stop_epoch": 0,
  "verbose_step": 50,
  "gradient_accumulation_steps": 1,
  "seed": 0,
  "q_update": false,
  "prediction_trans": false,
  "trans_drop": 0.5,
  "trans_heads": 3,
  "input_dim": 768,
  "model_gpu": "2,3/",
  "trained_weight": null,
  "type_lambda": 1,
  "sp_lambda": 5,
  "sp_threshold": 0.5,
  "label_type_num": 4,
  "max_query_len": 64,
  "max_doc_len": 512
}{
 "em": 0.6762376237623763,
 "f1": 0.7601174155630794,
 "prec": 0.7682573414079116,
 "recall": 0.7738104678610048,
 "sp_em": 0.4207920792079208,
 "sp_f1": 0.6294262283371184,
 "sp_prec": 0.6943234323432339,
 "sp_recall": 0.6142362093352192,
 "joint_em": 0.3415841584158416,
 "joint_f1": 0.5096671044187652,
 "joint_prec": 0.5584098819332566,
 "joint_recall": 0.5134729621437556
}{
 "em": 0.7079207920792079,
 "f1": 0.7998440034340178,
 "prec": 0.8062004941825596,
 "recall": 0.8148566678251675,
 "sp_em": 0.49207920792079207,
 "sp_f1": 0.654358721586443,
 "sp_prec": 0.7205775577557753,
 "sp_recall": 0.6331140971239984,
 "joint_em": 0.39801980198019804,
 "joint_f1": 0.5556970200544951,
 "joint_prec": 0.6114112092626914,
 "joint_recall": 0.5516820172305167
}{
 "em": 0.691089108910891,
 "f1": 0.7959602029462921,
 "prec": 0.8020618313402005,
 "recall": 0.8071104930527466,
 "sp_em": 0.48316831683168315,
 "sp_f1": 0.6481588158815866,
 "sp_prec": 0.7028194247996224,
 "sp_recall": 0.632982083922678,
 "joint_em": 0.3900990099009901,
 "joint_f1": 0.5419488329644467,
 "joint_prec": 0.588340770937805,
 "joint_recall": 0.5393818132165252
}{
 "em": 0.6861386138613862,
 "f1": 0.7898480527814845,
 "prec": 0.7967419533950442,
 "recall": 0.8048433240762007,
 "sp_em": 0.47128712871287126,
 "sp_f1": 0.6648121240695484,
 "sp_prec": 0.6909480590916227,
 "sp_recall": 0.6724045261669022,
 "joint_em": 0.3782178217821782,
 "joint_f1": 0.55135473992749,
 "joint_prec": 0.5744201159254542,
 "joint_recall": 0.570600766379467
}{
 "em": 0.7029702970297029,
 "f1": 0.8046542504223893,
 "prec": 0.8110266701218198,
 "recall": 0.8163751745067478,
 "sp_em": 0.49306930693069306,
 "sp_f1": 0.6663722800851505,
 "sp_prec": 0.7185502121640733,
 "sp_recall": 0.6522560113154177,
 "joint_em": 0.39900990099009903,
 "joint_f1": 0.5669258605340936,
 "joint_prec": 0.6097188465868809,
 "joint_recall": 0.5662083455056124
}{
 "em": 0.7029702970297029,
 "f1": 0.7970873262199946,
 "prec": 0.8052298828830375,
 "recall": 0.8114757459826539,
 "sp_em": 0.4910891089108911,
 "sp_f1": 0.6660173874530297,
 "sp_prec": 0.7069731258840164,
 "sp_recall": 0.660565770862801,
 "joint_em": 0.39900990099009903,
 "joint_f1": 0.5571022509109637,
 "joint_prec": 0.5936416249312263,
 "joint_recall": 0.5664386066016085
}{
 "em": 0.696039603960396,
 "f1": 0.7994883050492078,
 "prec": 0.8043801726494215,
 "recall": 0.8189147024342848,
 "sp_em": 0.48217821782178216,
 "sp_f1": 0.6650492192076335,
 "sp_prec": 0.7053488920320597,
 "sp_recall": 0.659964639321075,
 "joint_em": 0.38712871287128714,
 "joint_f1": 0.5576569149810585,
 "joint_prec": 0.5904785607622532,
 "joint_recall": 0.569941882483234
}{
 "em": 0.692079207920792,
 "f1": 0.798103543036244,
 "prec": 0.8037021345956825,
 "recall": 0.817292584178503,
 "sp_em": 0.48415841584158414,
 "sp_f1": 0.6708553712514098,
 "sp_prec": 0.7036185761433281,
 "sp_recall": 0.6723314474304571,
 "joint_em": 0.38613861386138615,
 "joint_f1": 0.5619455923148189,
 "joint_prec": 0.5902937181546396,
 "joint_recall": 0.5795074405416261
}{
  "name": "train_v1",
  "prediction_path": "output/submissions/train_v1",
  "checkpoint_path": "output/checkpoints/train_v1",
  "data_dir": "./data/output",
  "fp16": false,
  "ckpt_id": 0,
  "bert_model": "/data/wuyunzhao/pre-model/chinese_roberta_wwm_large_ext_pytorch",
  "epochs": 10,
  "qat_epochs": 0,
  "batch_size": 2,
  "max_bert_size": 8,
  "eval_batch_size": 2,
  "lr": 1e-05,
  "decay": 1.0,
  "early_stop_epoch": 0,
  "verbose_step": 50,
  "gradient_accumulation_steps": 1,
  "seed": 5,
  "q_update": false,
  "prediction_trans": false,
  "trans_drop": 0.5,
  "trans_heads": 3,
  "input_dim": 768,
  "model_gpu": "2,3",
  "trained_weight": null,
  "type_lambda": 1,
  "sp_lambda": 5,
  "sp_threshold": 0.5,
  "label_type_num": 4,
  "max_query_len": 64,
  "max_doc_len": 512
}{
  "name": "train_v1",
  "prediction_path": "output/submissions/train_v1",
  "checkpoint_path": "output/checkpoints/train_v1",
  "data_dir": "./data/output",
  "fp16": false,
  "ckpt_id": 0,
  "bert_model": "/data/wuyunzhao/pre-model/chinese_roberta_wwm_large_ext_pytorch",
  "epochs": 10,
  "qat_epochs": 0,
  "batch_size": 2,
  "max_bert_size": 8,
  "eval_batch_size": 2,
  "lr": 1e-05,
  "decay": 1.0,
  "early_stop_epoch": 0,
  "verbose_step": 50,
  "gradient_accumulation_steps": 1,
  "seed": 5,
  "q_update": false,
  "prediction_trans": false,
  "trans_drop": 0.5,
  "trans_heads": 3,
  "input_dim": 768,
  "model_gpu": "2,3",
  "trained_weight": null,
  "type_lambda": 1,
  "sp_lambda": 5,
  "sp_threshold": 0.5,
  "label_type_num": 4,
  "max_query_len": 64,
  "max_doc_len": 512
}{
 "em": 0.6653465346534654,
 "f1": 0.7563241695488561,
 "prec": 0.7635959145688949,
 "recall": 0.7688085561154792,
 "sp_em": 0.4158415841584158,
 "sp_f1": 0.5998842027059842,
 "sp_prec": 0.6955445544554452,
 "sp_recall": 0.5666195190947669,
 "joint_em": 0.3326732673267327,
 "joint_f1": 0.49182612231625666,
 "joint_prec": 0.5626932883568614,
 "joint_recall": 0.47724152044837287
}{
 "em": 0.689108910891089,
 "f1": 0.7784426417542688,
 "prec": 0.784120587941377,
 "recall": 0.7921635412743824,
 "sp_em": 0.4633663366336634,
 "sp_f1": 0.6196271055676985,
 "sp_prec": 0.700016501650165,
 "sp_recall": 0.5891041961338993,
 "joint_em": 0.38514851485148516,
 "joint_f1": 0.5192855416250981,
 "joint_prec": 0.5848713463963437,
 "joint_recall": 0.5054615741647059
}{
 "em": 0.6871287128712872,
 "f1": 0.7866871709507944,
 "prec": 0.7922922132686012,
 "recall": 0.7992859038799797,
 "sp_em": 0.48415841584158414,
 "sp_f1": 0.6597079707970787,
 "sp_prec": 0.7060183875530406,
 "sp_recall": 0.6512164073550208,
 "joint_em": 0.39702970297029705,
 "joint_f1": 0.5476863508991741,
 "joint_prec": 0.5828637814278568,
 "joint_recall": 0.5549226124972163
}{
 "em": 0.695049504950495,
 "f1": 0.7909030936574599,
 "prec": 0.7952789262469189,
 "recall": 0.804224305417416,
 "sp_em": 0.4752475247524752,
 "sp_f1": 0.6477920649207767,
 "sp_prec": 0.6871994342291368,
 "sp_recall": 0.6491796322489389,
 "joint_em": 0.38613861386138615,
 "joint_f1": 0.5399395310643776,
 "joint_prec": 0.5724087543245066,
 "joint_recall": 0.5532816702507143
}{
 "em": 0.7019801980198019,
 "f1": 0.7974847579012455,
 "prec": 0.8024238679095116,
 "recall": 0.8156289352349942,
 "sp_em": 0.48514851485148514,
 "sp_f1": 0.6557435743574341,
 "sp_prec": 0.7080858085808576,
 "sp_recall": 0.6429160773220176,
 "joint_em": 0.39504950495049507,
 "joint_f1": 0.5507034628199946,
 "joint_prec": 0.5918018391826274,
 "joint_recall": 0.5563095314927384
}{
 "em": 0.692079207920792,
 "f1": 0.7919140557931679,
 "prec": 0.7952513668871661,
 "recall": 0.8116052096382756,
 "sp_em": 0.4693069306930693,
 "sp_f1": 0.658610789650392,
 "sp_prec": 0.7108722300801502,
 "sp_recall": 0.648413484205563,
 "joint_em": 0.37722772277227723,
 "joint_f1": 0.5474833250429367,
 "joint_prec": 0.5904487873768919,
 "joint_recall": 0.5525326569949165
}{
 "em": 0.7108910891089109,
 "f1": 0.8046089872862541,
 "prec": 0.8120220864006725,
 "recall": 0.817848228000852,
 "sp_em": 0.49306930693069306,
 "sp_f1": 0.6618276827682752,
 "sp_prec": 0.709180810938236,
 "sp_recall": 0.6520344177274867,
 "joint_em": 0.39603960396039606,
 "joint_f1": 0.5554411784422342,
 "joint_prec": 0.5977606561089162,
 "joint_recall": 0.558982313071342
}{
 "em": 0.694059405940594,
 "f1": 0.7941475067001833,
 "prec": 0.8003388358439761,
 "recall": 0.8096640771599333,
 "sp_em": 0.45445544554455447,
 "sp_f1": 0.672569839401521,
 "sp_prec": 0.6937647336162179,
 "sp_recall": 0.6867138142385663,
 "joint_em": 0.36633663366336633,
 "joint_f1": 0.55607882970791,
 "joint_prec": 0.5750675198408528,
 "joint_recall": 0.5821526959262708
}{
 "em": 0.7079207920792079,
 "f1": 0.8074088849630079,
 "prec": 0.8121007174688232,
 "recall": 0.8227816283397,
 "sp_em": 0.48316831683168315,
 "sp_f1": 0.6658806594945197,
 "sp_prec": 0.7136056105610556,
 "sp_recall": 0.6572324375294669,
 "joint_em": 0.3930693069306931,
 "joint_f1": 0.5583977139405915,
 "joint_prec": 0.595990574960469,
 "joint_recall": 0.566547331930577
}{
 "em": 0.7029702970297029,
 "f1": 0.8020858014476987,
 "prec": 0.8067240619097941,
 "recall": 0.8181675955078455,
 "sp_em": 0.48118811881188117,
 "sp_f1": 0.6690925521123529,
 "sp_prec": 0.7099964639321071,
 "sp_recall": 0.6659453088165955,
 "joint_em": 0.3910891089108911,
 "joint_f1": 0.558369673614781,
 "joint_prec": 0.5908598772999795,
 "joint_recall": 0.5715195708616793
}